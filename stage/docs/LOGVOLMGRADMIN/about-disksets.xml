<chapter id="about-disksets-31856"><title>Disk
Sets (Overview)</title><highlights><para>This chapter provides conceptual information about disk sets. For information
about performing related tasks, see <olink targetptr="tasks-disksets-1" remap="internal">Chapter&nbsp;19,
Disk Sets (Tasks)</olink>.</para><para>This chapter includes the following information:</para><itemizedlist><listitem><para><olink targetptr="ftzhv" remap="internal">What's New in Disk Sets</olink></para>
</listitem><listitem><para><olink targetptr="about-disksets-39080" remap="internal">Introduction to Disk
Sets</olink></para>
</listitem><listitem><para><olink targetptr="about-disksets-39970" remap="internal">Solaris Volume Manager
Disk Set Administration</olink></para>
</listitem><listitem><para><olink targetptr="egjyp" remap="internal">Guidelines for Working With Disk
Sets</olink></para>
</listitem><listitem><para><olink targetptr="egjyq" remap="internal">Asynchronous Shared Storage in Disk
Sets</olink></para>
</listitem><listitem><para><olink targetptr="about-disksets-2" remap="internal">Scenario&mdash;Disk Sets</olink></para>
</listitem>
</itemizedlist>
</highlights><sect1 id="ftzhv"><title>What's New in Disk Sets</title><para>This section describes new disk set features in this Solaris release.</para><para>For a complete listing of new Solaris features and a description of
Solaris releases, see <olink targetdoc="nvdwhatsnew" remap="external"><citetitle remap="book">What&rsquo;s New in Solaris
Express</citetitle></olink>.</para><sect2 id="fvjxr"><title>Enhanced Output
With the <command>metaimport</command> Command</title><para><emphasis role="strong">Solaris Express 6/06</emphasis>: The <command>metaimport</command> command has been enhanced to allow you to import partial disk sets.
A partial disk set is a disk set that has at least one disk that is not available.
With this enhancement, a partial disk set can still be imported provided one
disk with a <literal>metadb</literal> is present in the set.</para><para>This new functionality is associated with enhancements in the <command>metaimport</command> <option>r</option> command. The command's output now includes the
following information:</para><variablelist><varlistentry><term>Time stamp</term><listitem><para>The time the disk set is created is displayed.</para>
</listitem>
</varlistentry><varlistentry><term>Unavailable disks</term><listitem><para>If a disk has become unresponsive or might have been removed
from the system, the disk is labeled unavailable.</para>
</listitem>
</varlistentry><varlistentry><term>Disk conflicts</term><listitem><para>If a disk is listed as belonging to more than one disk set,
the disk is marked as in conflict.</para>
</listitem>
</varlistentry><varlistentry><term>Import recommendation</term><listitem><para>If disk conflicts exist, <command>metaimport</command> recommends
which disk set to import with the disk in conflict.</para>
</listitem>
</varlistentry>
</variablelist><para>The information generated by <command>metaimport</command> <option>r</option> helps
reduce the risk of data corruption when importing disk sets. In cases of disk
conflicts, <command>metaimport</command> imports only the most recent disk
set that is created with the disk in conflict. If a user imports a disk set
with time conflicts contrary to the recommendation, the attempt is blocked.</para><para>The <command>metaimport</command> <option>rv</option> command now contains
more information in the output. This information allows the user to see the
basic structure of the volumes that are available for import.</para><para>Finally, you can import the recommended partial disk set by using the
following syntax:</para><para><userinput>metaimport -s <replaceable>setname</replaceable> -f <replaceable>diskname</replaceable></userinput></para><para>For examples of the output of the <command>metaimport <option>r</option></command> and <command>metaimport <option>rv</option></command> commands, see <olink targetptr="eoqsa" remap="internal">How
to Print a Report on Disk Sets Available for Import</olink>.</para>
</sect2>
</sect1><sect1 id="about-disksets-39080"><title>Introduction to Disk Sets</title><para>A disk set is a set of physical storage volumes that contain logical
volumes and hot spares. Volumes and hot spare pools must be built on drives
from within that disk set. Once you have created a volume within the disk
set, you can use the volume just as you would use a physical slice. You can
use the volume to create and to mount a file system and to store data.</para><note><para>Disk sets are supported on both SPARC and x86 based platforms.</para>
</note>
</sect1><sect1 id="eqqda"><title>Types of Disk Sets</title><para>This section discusses the different types of disk sets available in Solaris Volume Manager.</para><sect2 id="eqbhy"><title>Local Disk Sets</title><para>Each host has a local disk set. The local disk set consists of all of
the disks on a host that are not in a named disk set. A local disk set belongs
exclusively to a specific host. The local disk set contains the state database
for that specific host's configuration. Volumes and hot spare pools in the
local disk set consist only of drives from within the local disk set.</para>
</sect2><sect2 id="eqbia"><title>Named Disk Sets</title><para>In addition to local disk sets, hosts can participate in named disk
sets. A named disk set is any disk set that is not in the local disk set.
You can implement the following types of named disk sets to manage volumes,
depending on the configuration of your system.</para><sect3 id="eqbib"><title>Shared Disk Sets</title><para>A <emphasis>shared disk set</emphasis> can be shared by multiple hosts.
Although a shared disk set is visible from all the participating hosts, only
the owner of the disk set can access it. Each host can control a shared disk
set, but only one host can control it at a time. Additionally, shared disk
sets provide a distinct namespace within which the volume is managed.</para><para>A shared disk set supports data redundancy and data availability. If
one host fails, another host can take over the failed host's disk set (this
type of configuration is known as a <emphasis>failover configuration</emphasis>).</para><note><para>Shared disk sets are intended, in part, for use with Sun Cluster,
Solstice HA (High Availability), or another supported third-party HA framework.
Solaris Volume Manager by itself does not provide all the functionality necessary
to implement a failover configuration.</para>
</note><para>Although each host can control the set of disks, only one host can control
it at a time.</para>
</sect3><sect3 id="eqbhz"><title>Autotake Disk Sets</title><para>Before the autotake feature became available in the Solaris 9 4/04 release, Solaris Volume Manager did
not support the automatic mounting of file systems on disk sets through the <literal>/etc/vfstab</literal> file. Solaris Volume Manager required the system administrator
to manually issue a disk set take command by using the <command>metaset <option>s</option> <replaceable>setname</replaceable> <option>t</option></command> command
before the file systems on the disk set could be accessed.</para><para>With the autotake feature, you can set a disk set to be automatically
accessed at boot time by using the <command>metaset</command> <option>s</option> <replaceable>setname</replaceable> <option>A</option> <literal>enable</literal> command.
The autotake feature makes it possible for you to define at boot the mount
options for a file system in the <literal>/etc/vfstab</literal> file. This
feature allows you to define the mount options in the <literal>/etc/vfstab</literal> file
for file systems on volumes in the enabled disk set.</para><para>Only single-host disk sets support the autotake feature. The autotake
feature requires that the disk set is not shared with any other systems. A
disk set that is shared cannot be set to use the autotake feature, and the <command>metaset</command> <option>A</option> command will fail. However, after other
hosts are removed from the disk set, it may then be set to autotake. Similarly,
an autotake disk set cannot have other hosts added to it. If the autotake
feature is disabled, additional hosts can then be added to the disk  set. </para><note><para>In a Sun Cluster environment, the autotake feature is disabled.
Sun Cluster handles the take and release of a disk set.</para>
</note><para>For more information on the autotake feature see the <option>A</option> option
of the <olink targetdoc="refman1m" targetptr="metaset-1m" remap="external"><citerefentry><refentrytitle>metaset</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> command.</para>
</sect3><sect3 id="eqbid"><title>Multi-Owner Disk Sets</title><para>Named disk sets created in a Sun Cluster environment are called multi-owner
disk sets. Multi-owner disk sets allow multiple nodes to share the ownership
of the disk sets and to simultaneously access the shared disks. All disks
and volumes in a multi-owner disk set can be directly accessed by all the
nodes in a cluster. Each multi-owner disk set contains a list of hosts that
have been added to the disk set. Consequently, each multi-owner disk set within
a cluster configuration can have a different (and sometimes overlapping) set
of hosts.</para><para>Each multi-owner disk set has a master node. The function of the master
node is to manage and update the state database replica changes. Since there
is a master node per disk set, multiple masters can exist simultaneously.
There are two ways that the master is chosen. The first way is that a node
becomes the master if it is the first node to add a disk to the disk set.
The second way is when a master node panics and fails. The node with the lowest
node id becomes the master node.</para><para>Multi-owner disk set functionality is enabled only in a Sun Cluster
environment to manage multi-owner disk set storage. The Solaris Volume Manager for Sun Cluster feature works
with releases of Sun Cluster beginning with the Sun Cluster 10/04 software
collection and with applications like Oracle Real Applications Clusters. For
more information on Solaris Volume Manager for Sun Cluster, see Chapter&nbsp;4, Solaris Volume Manager for
Sun Cluster (Overview).</para><para>Before you can configure multi-owner disk sets, the following software
must be installed in addition to the Solaris OS:</para><itemizedlist><listitem><para>Sun Cluster initial cluster framework</para>
</listitem><listitem><para>Sun Cluster Support for Oracle Real Application Clusters software</para>
</listitem><listitem><para>Oracle Real Application Clusters software</para>
</listitem>
</itemizedlist><note><para>For information on setting up Sun Cluster and Oracle Real Application
Clusters software, see <olink targetdoc="clustinstall" remap="external"><citetitle remap="book">Sun
Cluster Software Installation Guide for Solaris OS</citetitle></olink> and <olink targetdoc="sc31ds403opsrac" remap="external"><citetitle remap="book">Sun Cluster Data Service
for Oracle Real Application Clusters Guide for Solaris OS</citetitle></olink>.</para>
</note>
</sect3>
</sect2>
</sect1><sect1 id="about-disksets-39970"><title>Solaris Volume Manager Disk Set Administration</title><para>Unlike local disk set administration, you do not need to manually create
or delete disk set state databases. Solaris Volume Manager places one state database
replica (on slice 7) on each disk across all disks in the disk set, up to
a maximum of 50 total replicas in the disk set.</para><para>When you add disks to a disk set, Solaris Volume Manager automatically creates
the state database replicas on the disk set. When a disk is accepted into
a disk set, Solaris Volume Manager might repartition the disk so that the state database
replica for the disk set can be placed on the disk (see <olink targetptr="about-disksets-123" remap="internal">Automatic Disk Partitioning</olink>).</para><para>A file system that resides on a volume in a disk set normally is not
mounted automatically at boot time with the <filename>/etc/vfstab</filename> file.
The necessary Solaris Volume Manager RPC daemons (<filename>rpc.metad</filename> and <filename>rpc.metamhd</filename>) do not start early enough in the boot process to permit
this. Additionally, the ownership of a disk set is lost during a reboot. Do
not disable the Solaris Volume Manager RPC daemons in the <filename>/etc/inetd.conf</filename> file.
They are configured to start by default. These daemons must remain enabled
to allow Solaris Volume Manager to use its full functionality.</para><para>When the autotake feature is enabled using the <option>A</option> option
of the <command>metaset</command> command, the disk set is automatically taken
at boot time. Under these circumstances, a file system that resides on a volume
in a disk set can be automatically mounted with the <filename>/etc/vfstab</filename> file.
To enable an automatic take during the boot process, the disk set must be
associated with only a single host, and must have the autotake feature enabled.
A disk set can be enabled either during or after disk set creation. For more
information on the autotake feature, see <olink targetptr="eqbhz" remap="internal">Autotake
Disk Sets</olink>.</para><note><para>Although disk sets are supported in single-host configurations,
they are often not appropriate for &ldquo;local&rdquo; (not dual-connected)
use. Two common exceptions are the use of disk sets to provide a more manageable
namespace for logical volumes, and to more easily manage storage on a Storage
Area Network (SAN) fabric (see <olink targetptr="about-disksets-2" remap="internal">Scenario&mdash;Disk
Sets</olink>.</para>
</note><para>Disk sets can be created and configured by using the Solaris Volume Manager command-line
interface (the <command>metaset</command> command) or the Enhanced Storage tool within the Solaris Management Console.  </para><para>After disks are added to a disk set, the disk set can be <emphasis>reserved</emphasis> (or <emphasis>taken</emphasis>) and <emphasis>released</emphasis> by
hosts in the disk set. When a disk set is reserved by a host, the other host
in the disk set cannot access the data on the disks in the disk set. To perform
maintenance on a disk set, a host must be the owner of the disk set or have
reserved the disk set. A host takes implicit ownership of the disk set by
putting the first disk into the set.</para><para>Disk sets, including disk sets created on a different system, can be
imported into existing Solaris Volume Manager configurations using the <command>metaimport</command> command.</para><sect2 id="eqbic"><title>Reserving a Disk Set</title><para>Before a host can use the disks in a disk set, the host must reserve
the disk set. There are two methods of reserving a disk set: </para><itemizedlist><listitem><para><emphasis role="strong">Safely -</emphasis> When you safely
reserve a disk set, Solaris Volume Manager attempts to take the disk set, and the
other host attempts to release the disk set. The release (and therefore the
reservation) might fail. </para>
</listitem><listitem><para><emphasis role="strong">Forcibly -</emphasis> When you forcibly
reserve a disk set, Solaris Volume Manager reserves the disk set whether or not another
host currently has the set reserved. This method is generally used when a
host in the disk set is down or not communicating. All disks within the disk
set are taken over. The state database is read in on the host performing the
reservation and the shared volumes configured in the disk set become accessible.
If the other host had the disk set reserved at this point, it would panic
due to reservation loss.</para><para>Normally, two hosts in a disk set cooperate
with each other to ensure that the disks in a disk set are reserved by only
one host at a time. A normal situation is defined as both hosts being up and
communicating with each other.</para>
</listitem>
</itemizedlist><note><para>If a disk has been determined unexpectedly not to be reserved
(perhaps because another host using the disk set forcibly took the disk),
the host will panic. This behavior helps to minimize data loss which would
occur if two hosts were to simultaneously access the same disk.</para>
</note><para>For more information about taking or reserving a disk set, see <olink targetptr="tasks-disksets-3" remap="internal">How to Take a Disk Set</olink>.</para>
</sect2><sect2 id="eqbij"><title>Releasing a Disk Set</title><para>Releasing a disk set can be useful when you perform maintenance on the
physical disks in the disk set. When a disk set is released, it cannot be
accessed by the host. If both hosts in a disk set release the set, neither
host in the disk set can access the disks in the disk set.</para><para>For more information about releasing a disk set, see <olink targetptr="tasks-disksets-6" remap="internal">How to Release a Disk Set</olink>.</para>
</sect2><sect2 id="eqbji"><title>Importing a Disk Set</title><para>Beginning with the Solaris 9 9/04 release, the <command>metaimport</command> command
enables you to import disk sets, including replicated disk sets, into existing
Solaris Volume Manager configurations that have device ID support in the disk
set. You can also use the <command>metaimport</command> command to report
on disk sets that are available for import.</para><para>Replicated disk sets are created using remote replication software.
In order for a replicated disk set to be imported with the <command>metaimport</command> command,
the slice containing the state database replica for each disk in the disk
set must also be replicated onto the same slice of the replicated disk set.
This corresponds to slice 7 for non-EFI disks and slice 6 for EFI disks. Before
replicating a disk set, make sure that the disk configurations of the data
to be replicated and of the remote site match. This step ensures that both
the state database replica and the data are replicated accurately.</para><para>The <command>metaimport</command> command also does not import a disk
in a disk set if the disk does not contain a volume or a state database replica.
This scenario occurs if a volume or a state database replica have not been
added to a disk or have been deleted from a disk. In this case, when you import
the disk set to another system, you would find that the disk is missing from
the disk set. For example, maximum of 50 state database replicas are allowed
per Solaris Volume Manager disk set. If you have 60 disks in a disk set, the 10 disks
that do not contain a state database replica must contain a volume in order
to be imported with the disk set.</para><para>For tasks associated with importing a disk set, see <olink targetptr="efnri" remap="internal">Importing Disk Sets</olink>.</para><note><para>In a Sun Cluster environment, the <command>metaimport</command> command
is not available.</para>
</note>
</sect2><sect2 id="about-disksets-123"><title>Automatic Disk Partitioning</title><para>When you add a new disk to a disk set, Solaris Volume Manager checks the disk
format. If necessary, Solaris Volume Manager repartitions the disk to ensure that the
disk has an appropriately configured slice 7 with adequate space for a state
database replica. The precise size of slice 7 depends on the disk geometry.
However, the size will be no less than 4 Mbytes, and probably closer to 6
Mbytes (depending on where the cylinder boundaries lie). </para><para>By default, Solaris Volume Manager places one state database replica on slice
7. You can increase the default size of slice 7 or decrease the size of the
state database replica in order to fit more than one state database replica
onto the slice.</para><note><para>The minimal size for slice 7 will likely change in the future,
based on a variety of factors, including the size of the state database replica
and information to be stored in the state database replica. The default size
for a state database replica in a multi-owner disk set is 16 Mbytes.</para>
</note><para>For use in disk sets, disks must have a slice 7 that meets these criteria:</para><itemizedlist><listitem><para>Starts at sector 0</para>
</listitem><listitem><para>Includes enough space for a disk label and state database
replicas</para>
</listitem><listitem><para>Cannot be mounted</para>
</listitem><listitem><para>Does not overlap with any other slices, including slice 2</para>
</listitem>
</itemizedlist><para>If the existing partition table does not meet these criteria, Solaris Volume Manager repartitions
the disk. A small portion of each drive is reserved in slice 7 for use by Solaris Volume Manager.
The remainder of the space on each drive is placed into slice 0. Any existing
data on the disks is lost by repartitioning. </para><tip><para>After you add a drive to a disk set, you may repartition it  as
necessary, with the exception that slice 7 is not altered in any way. </para>
</tip><para>The following output from the <command>prtvtoc</command> command shows
a disk before it is added to a disk set.</para><screen>[root@lexicon:apps]$ prtvtoc /dev/rdsk/c1t6d0s0
* /dev/rdsk/c1t6d0s0 partition map
*
* Dimensions:
*     512 bytes/sector
*     133 sectors/track
*      27 tracks/cylinder
*    3591 sectors/cylinder
*    4926 cylinders
*    4924 accessible cylinders
*
* Flags:
*   1: unmountable
*  10: read-only
*
*                          First     Sector    Last
* Partition  Tag  Flags    Sector     Count    Sector  Mount Directory
       0      2    00          0   4111695   4111694
       1      3    01    4111695   1235304   5346998
       2      5    01          0  17682084  17682083
       3      0    00    5346999   4197879   9544877
       4      0    00    9544878   4197879  13742756
       5      0    00   13742757   3939327  17682083</screen><para>The above output shows that the disk does not contain a slice 7. Therefore,
when the disk is added to a disk set, Solaris Volume Manager repartitions the disk.
The following output from the <command>prtvtoc</command> command shows the
disk after it is added to a disk set.</para><screen>[root@lexicon:apps]$ prtvtoc /dev/rdsk/c1t6d0s0
* /dev/rdsk/c1t6d0s0 partition map
*
* Dimensions:
*     512 bytes/sector
*     133 sectors/track
*      27 tracks/cylinder
*    3591 sectors/cylinder
*    4926 cylinders
*    4924 accessible cylinders
*
* Flags:
*   1: unmountable
*  10: read-only
*
*                          First     Sector    Last
* Partition  Tag  Flags    Sector     Count    Sector  Mount Directory
       0      0    00      10773  17671311  17682083
<emphasis role="strong">       7      0    01          0     10773     10772</emphasis>
[root@lexicon:apps]$ </screen><para>The output shows that the disk has been repartitioned to include a slice
7 that starts at cylinder 0 and that has sufficient space for the state database
replica. If disks you add to a disk set each have an acceptable slice 7, they
are not reformatted.</para><note><para>If you have disk sets that you upgraded from Solstice DiskSuite
software, the default state database replica size on those sets is 1034 blocks,
not the 8192 block size from Solaris Volume Manager. Also, slice 7 on the disks that
were added under Solstice DiskSuite software are correspondingly smaller than
slice 7 on disks that were added under Solaris Volume Manager. </para>
</note>
</sect2><sect2 id="basics-disk-set-name-requirements"><title>Disk Set Name Requirements</title><para>Disk set volume names are similar to other Solaris Volume Manager component
names. However, the disk set name is included as part of the name. For example,
volume path names include the disk set name after <filename>/dev/md/</filename> and
before the actual volume name in the path.</para><para>The following table shows some example disk set volume names.</para><table frame="topbot" id="basics-disk-set-names"><title>Example Volume Names
for Disk Sets</title><tgroup cols="2" colsep="0" rowsep="0"><colspec colnum="1" colname="column1" colwidth="4*"/><colspec colnum="2" colname="column2" colwidth="4*"/><tbody><row><entry><para><filename>/dev/md/blue/dsk/d0</filename></para>
</entry><entry><para>Block volume <literal>d0</literal> in disk set <literal>blue</literal></para>
</entry>
</row><row><entry><para><filename>/dev/md/blue/dsk/d1</filename></para>
</entry><entry><para>Block volume <literal>d1</literal> in disk set <literal>blue</literal></para>
</entry>
</row><row><entry><para><filename>/dev/md/blue/rdsk/d126</filename></para>
</entry><entry><para>Raw volume <literal>d126</literal> in disk set <literal>blue</literal></para>
</entry>
</row><row><entry><para><filename>/dev/md/blue/rdsk/d127</filename></para>
</entry><entry><para>Raw volume <literal>d127</literal> in disk set <literal>blue</literal></para>
</entry>
</row>
</tbody>
</tgroup>
</table><para>Similarly, hot spare pools have the disk set name as part of the hot
spare name.</para>
</sect2><sect2 id="about-disksets-1"><title>Example&mdash;Two Shared Disk Sets</title><para><olink targetptr="about-disksets-fig-2" remap="internal">Figure 18&ndash;1</olink> shows
an example configuration that uses two disk sets. </para><para>In this configuration, Host A and Host B share the disk sets red and
blue. They each have their own local disk set, which is not shared. If Host
A fails, Host B can take over control of Host A's shared disk set, the disk
set red. Likewise, if Host B fails, Host A can take control of Host B's shared
disk set, the disk set blue.</para><figure id="about-disksets-fig-2"><title>Disk Sets Example</title><mediaobject><imageobject><imagedata entityref="disksets-figure.tif"/>
</imageobject><textobject><simpara>Diagram shows how two hosts can share some disks through
shared disk sets and retain exclusive use of other disks in local disk sets. </simpara>
</textobject>
</mediaobject>
</figure>
</sect2>
</sect1><sect1 id="egjyp"><title>Guidelines for Working With Disk Sets</title><para>When working with disk sets, consider the following guidelines:</para><itemizedlist><listitem><para>Solaris Volume Manager must be configured on each host that will
be connected to the disk set.</para>
</listitem><listitem><para>Each host must have its local state database set up before
you can create disk sets.</para>
</listitem><listitem><para>The sequence of steps for creating a disk set and creating
the volumes for that disk set is to, first, create the disk set. Then, add
the disks to the disk set. Finally, create the volumes in the disk set.</para>
</listitem><listitem><para>To create and work with a disk set in a clustering environment, <literal>root</literal> must be a member of Group 14. Alternatively, the <filename>/.rhosts</filename> file on each host must contain an entry for the other host names
associated with the disk set.</para><note><para>This step is not necessary in a SunCluster 3.x environment.</para>
</note>
</listitem><listitem><para>To perform maintenance on a disk set, a host must be the owner
of the disk set or have reserved the disk set. A host takes implicit ownership
of the disk set by putting the first drives into the disk set.</para>
</listitem><listitem><para>You cannot add a drive to a disk set that is in use for a
file system, database or any other application. Before you add a drive, make
sure that it is not currently being used.</para>
</listitem><listitem><para>Do not add to a disk set a drive containing existing data
that you want to preserve. The process of adding the disk to the disk set
repartitions the disk and destroys existing data.</para>
</listitem><listitem><para>Unlike local volume administration, it is not necessary to
manually create or delete state database replicas on the disk set. Solaris Volume Manager tries
to balance a reasonable number of state database replicas across all drives
in a disk set.</para>
</listitem><listitem><para>When drives are added to a disk set, Solaris Volume Manager rebalances
the state database replicas across the remaining drives. Later, if necessary,
you can change the replica layout with the <command>metadb</command> command.</para>
</listitem>
</itemizedlist>
</sect1><sect1 id="egjyq"><title>Asynchronous Shared Storage in Disk Sets</title><para>In previous versions of Solaris Volume Manager, all of the disks that
you planned to share between hosts in the disk set had to be connected to
each host. They also had to have the exact same path, driver, and name on
each host. Specifically, a shared disk drive had to be seen by both hosts
in the same location (<literal>/dev/rdsk/c#t#d#</literal>). In addition, the
shared disks had to use the same driver name (<literal>ssd</literal>).</para><para>In the current Solaris OS release,  systems that have different views
of commonly accessible storage can nonconcurrently share access to a disk
set. With the introduction of device ID support for disk sets, Solaris Volume
Manager automatically tracks disk movement within named disk sets.</para><note><para>Device ID support for disk sets is not supported in a Sun Cluster
environment.</para>
</note><para>When you upgrade to the latest Solaris OS, you need to take the disk
set once in order to enable disk tracking. For more information on taking
a disk set, see <olink targetptr="tasks-disksets-3" remap="internal">How to Take a Disk Set</olink>. </para><para>If the autotake feature is not enabled, you have to take each disk set
manually.  If this feature is enabled, this step is done automatically when
the system is rebooted. For more information on the autotake feature, see <olink targetptr="eqbhz" remap="internal">Autotake Disk Sets</olink>.</para><para>This expanded device ID support also enables you to import disk sets,
even disk sets that were created on different systems. For more information
on importing disk sets, see <olink targetptr="eqbji" remap="internal">Importing a Disk Set</olink>.</para>
</sect1><sect1 id="about-disksets-2"><title>Scenario&mdash;Disk Sets</title><para>The following example, drawing on the sample system shown in <olink targetptr="svm-scenario-1" remap="internal">Chapter&nbsp;5, Configuring and Using Solaris Volume
Manager (Scenario)</olink>, describes how disk sets should be used to manage
storage that resides on a SAN (Storage Area Network) fabric. </para><para>Assume that the sample system has an additional controller that connects
to a fiber switch and SAN storage. Storage on the SAN fabric is unavailable
to the system as early in the boot process as other devices, such as SCSI
and IDE disks. In addition, Solaris Volume Manager would report logical volumes on
the fabric as unavailable at boot. However, by adding the storage to a disk
set, and then using the disk set tools to manage the storage, this problem
with boot time availability is avoided. Also, the fabric-attached storage
can be easily managed within a separate, disk set-controlled, namespace from
the local storage.</para>
</sect1>
</chapter>
<chapter id="zfsover-1"><title>Solaris ZFS
File System (Introduction)</title><highlights><para>This chapter provides an overview of the <trademark>Solaris</trademark> ZFS
file system and its features and benefits. This chapter also covers some basic
terminology used throughout the rest of this book.</para><para>The following sections are provided in this chapter:</para><itemizedlist><listitem><para><olink targetptr="gbscy" remap="internal">What's New in ZFS?</olink></para>
</listitem><listitem><para><olink targetptr="zfsover-2" remap="internal">What Is ZFS?</olink></para>
</listitem><listitem><para><olink targetptr="ftyue" remap="internal">ZFS Terminology</olink></para>
</listitem><listitem><para><olink targetptr="gbcpt" remap="internal">ZFS Component Naming Requirements</olink></para>
</listitem>
</itemizedlist>
</highlights><sect1 id="gbscy"><title>What's New in ZFS?</title><para>This section summarizes new features in the ZFS file system.</para><itemizedlist><listitem><para><olink targetptr="gfxul" remap="internal">Using
Cache Devices in Your ZFS Storage Pool</olink></para>
</listitem><listitem><para><olink targetptr="gfwqj" remap="internal">Enhancements to the zfs send Command</olink></para>
</listitem><listitem><para><olink targetptr="gfwpz" remap="internal">ZFS Quotas and Reservations for File System
Data Only</olink></para>
</listitem><listitem><para><olink targetptr="gftgg" remap="internal">ZFS
File System Properties for the Solaris CIFS Service</olink></para>
</listitem><listitem><para><olink targetptr="gftgp" remap="internal">ZFS Storage Pool Properties</olink></para>
</listitem><listitem><para><olink targetptr="gfwqw" remap="internal">ZFS and File System Mirror
Mounts</olink></para>
</listitem><listitem><para><olink targetptr="gfiit" remap="internal">ZFS Command History Enhancements
(zpool history)</olink></para>
</listitem><listitem><para><olink targetptr="gffys" remap="internal">Upgrading ZFS File Systems (zfs upgrade)</olink></para>
</listitem><listitem><para><olink targetptr="gfgak" remap="internal">ZFS Delegated Administration</olink></para>
</listitem><listitem><para><olink targetptr="gfgaa" remap="internal">Setting Up Separate ZFS Logging Devices</olink></para>
</listitem><listitem><para><olink targetptr="gfgam" remap="internal">Creating Intermediate ZFS Datasets</olink></para>
</listitem><listitem><para><olink targetptr="gfgac" remap="internal">ZFS Hotplugging Enhancements</olink></para>
</listitem><listitem><para><olink targetptr="gevnp" remap="internal">Recursively Renaming ZFS Snapshots
(zfs rename -r)</olink></para>
</listitem><listitem><para><olink targetptr="gevpi" remap="internal">GZIP Compression is Available for
ZFS</olink></para>
</listitem><listitem><para><olink targetptr="gevpg" remap="internal">Storing Multiple Copies of ZFS User
Data</olink></para>
</listitem><listitem><para><olink targetptr="gebws" remap="internal">Improved zpool status Output</olink></para>
</listitem><listitem><para><olink targetptr="gebwq" remap="internal">ZFS and Solaris iSCSI Improvements</olink></para>
</listitem><listitem><para><olink targetptr="gebwp" remap="internal">Sharing ZFS File System Enhancements</olink></para>
</listitem><listitem><para><olink targetptr="gdswe" remap="internal">ZFS Command History (zpool history)</olink></para>
</listitem><listitem><para><olink targetptr="gdswf" remap="internal">ZFS Property Improvements</olink></para>
</listitem><listitem><para><olink targetptr="gdsvh" remap="internal">Displaying All ZFS File System Information</olink></para>
</listitem><listitem><para><olink targetptr="gdsup" remap="internal">New zfs receive -F Option</olink></para>
</listitem><listitem><para><olink targetptr="gdfdt" remap="internal">Recursive ZFS Snapshots</olink></para>
</listitem><listitem><para><olink targetptr="gcviu" remap="internal">Double Parity RAID-Z (raidz2)</olink></para>
</listitem><listitem><para><olink targetptr="gcvdm" remap="internal">Hot Spares for ZFS Storage Pool Devices</olink></para>
</listitem><listitem><para><olink targetptr="gcvgc" remap="internal">Replacing a ZFS File System With
a ZFS Clone (zfs promote)</olink></para>
</listitem><listitem><para><olink targetptr="gcvit" remap="internal">Upgrading ZFS Storage Pools (zpool
upgrade)</olink></para>
</listitem><listitem><para><olink targetptr="gcsxk" remap="internal">Using ZFS to Clone Non-Global Zones
and Other Enhancements</olink></para>
</listitem><listitem><para><olink targetptr="gciui" remap="internal">ZFS Backup and Restore Commands are
Renamed</olink></para>
</listitem><listitem><para><olink targetptr="gcitn" remap="internal">Recovering Destroyed Storage Pools</olink></para>
</listitem><listitem><para><olink targetptr="gcfhy" remap="internal">ZFS is Integrated With Fault Manager</olink></para>
</listitem><listitem><para><olink targetptr="gcfiw" remap="internal">New zpool clear Command</olink></para>
</listitem><listitem><para><olink targetptr="gcajn" remap="internal">Compact NFSv4 ACL Format</olink></para>
</listitem><listitem><para><olink targetptr="gcakl" remap="internal">File System Monitoring Tool (fsstat)</olink></para>
</listitem><listitem><para><olink targetptr="gbsbp" remap="internal">ZFS Web-Based Management</olink></para>
</listitem>
</itemizedlist><sect2 id="gfxul"><title>Using Cache Devices in Your ZFS Storage Pool</title><para><emphasis role="strong">Solaris Express Developer Edition 1/08:</emphasis> In
this Solaris release, you can create pool and specify <emphasis>cache devices</emphasis>,
which are used to cache storage pool data.</para><para>Cache devices provide an additional layer of caching between main memory
and disk. Using cache devices provide the greatest performance improvement
for random read-workloads of mostly static content.</para><para>One or more cache devices can specified when the pool is created. For
example:</para><screen># zpool create pool mirror c0t2d0 c0t4d0 cache c0t0d0
# zpool status pool
  pool: pool
 state: ONLINE
 scrub: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        pool        ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c0t2d0  ONLINE       0     0     0
            c0t4d0  ONLINE       0     0     0
        cache
          c0t0d0    ONLINE       0     0     0

errors: No known data errors</screen><para>After cache devices are added, they gradually fill with content from
main memory. Depending on the size of your cache device, it could take over
an hour for them to fill. Capacity and reads can be monitored by using the <command>zpool iostat</command> command as follows:</para><screen># zpool iostat -v pool 5</screen><para>Cache devices can be added or removed from the pool after the pool is
created.</para><para>For more information, see <olink targetptr="gfxtd" remap="internal">Creating a ZFS Storage
Pool with Cache Devices</olink> and <olink targetptr="gfxrx" remap="internal">Example&nbsp;4&ndash;3</olink>.</para>
</sect2><sect2 id="gfwqj"><title>Enhancements to the <command>zfs send</command> Command</title><para><emphasis role="strong">Solaris Express Developer Edition 1/08:</emphasis> This
release includes the following enhancements to the <command>zfs send</command> command.</para><itemizedlist><listitem><para>Send all incremental streams from one snapshot to a cumulative
snapshot. For example:</para><screen># zfs list
NAME                      USED  AVAIL  REFER  MOUNTPOINT
pool                      428K  16.5G    20K  /pool
pool/fs                    71K  16.5G    21K  /pool/fs
pool/fs@snapA              16K      -  18.5K  -
pool/fs@snapB              17K      -    20K  -
pool/fs@snapC              17K      -  20.5K  -
pool/fs@snapD                0      -    21K  -
# zfs send -I pool/fs@snapA pool/fs@snapD > /snaps/fs@combo</screen><para>Send all incremental snapshots between <filename>fs@snapA</filename> to <filename>fs@snapD</filename> to <filename>fs@combo</filename>.</para>
</listitem><listitem><para>Send an incremental stream from the origin snapshot to create
a clone. The original snapshot must already exist on the receiving side to
accept the incremental stream. For example:</para><screen># zfs send -I pool/fs@snap1 pool/clone@snapA > /snaps/fsclonesnap-I
.
.
# zfs receive -F pool/clone &lt; /snaps/fsclonesnap-I</screen>
</listitem><listitem><para>Send a replication stream of all descendent file systems,
up to the named snapshots.  When received, all properties, snapshots, descendent
file systems, and clones are preserved. For example:</para><screen>zfs send -R pool/fs@snap > snaps/fs-R</screen><para>For an extended example, see <olink targetptr="gfxpm" remap="internal">Example&nbsp;6&ndash;1</olink>.</para>
</listitem><listitem><para>Send an incremental replication stream. </para><screen>zfs send -R -[iI] @snapA pool/fs@snapD</screen><para>For an extended example, see <olink targetptr="gfxpm" remap="internal">Example&nbsp;6&ndash;1</olink>.</para>
</listitem>
</itemizedlist><para>For more information, see <olink targetptr="gfwqb" remap="internal">Sending and Receiving
Complex ZFS Snapshot Streams</olink>.</para>
</sect2><sect2 id="gfwpz"><title>ZFS Quotas and Reservations for File System Data
Only</title><para><emphasis role="strong">Solaris Express Developer Edition 1/08:</emphasis> In
addition to the existing ZFS quota and reservation features, this release
includes dataset quotas and reservations that do not include descendents,
such as snapshots and clones, in the space consumption accounting.</para><itemizedlist><listitem><para>The <property>refquota</property> property limits the amount
of space a dataset can consume. This property enforces a hard limit on the
amount of space that can be used. This hard limit does not include space used
by descendents, such as snapshots and clones.</para>
</listitem><listitem><para>The <literal>refreservation</literal> property sets the minimum
amount of space that is guaranteed to a dataset, not including its descendents.</para>
</listitem>
</itemizedlist><para>For example, you can set a 10 Gbyte <property>refquota</property> for <literal>studentA</literal> that sets a 10-Gbyte hard limit of <emphasis>referenced</emphasis> space.
For additional flexibility, you can set a 20-Gbyte quota that allows you to
manage <literal>studentA</literal>'s snapshots.</para><screen># zfs set refquota=10g tank/studentA
# zfs set quota=20g tank/studentA</screen><para>For more information, see <olink targetptr="gazvb" remap="internal">ZFS Quotas and Reservations</olink>.</para>
</sect2><sect2 id="gftgg"><title>ZFS File System Properties for the Solaris CIFS Service</title><para><emphasis role="strong">Solaris Express Developer Edition 1/08:</emphasis> This
release provides support for the Solaris Common Internet
File System (CIFS) service. This product provides the ability to share files
between Solaris and Windows or MacOS systems.</para><para>To facilitate sharing files between
these systems by using the Solaris CIFS service, the following new ZFS properties
are provided:</para><itemizedlist><listitem><para>Case sensitivity support (<literal>casesensitivity</literal>)</para>
</listitem><listitem><para>Non-blocking mandatory locks (<literal>nbmand</literal>)</para>
</listitem><listitem><para>SMB share support (<literal>sharesmb</literal>)</para>
</listitem><listitem><para>Unicode normalization support (<literal>normalization</literal>)</para>
</listitem><listitem><para>UTF-8 character set support (<literal>utf8only</literal>)</para>
</listitem>
</itemizedlist><para>Currently, the <literal>sharesmb</literal> property is available to
share ZFS files in the Solaris CIFS environment. More ZFS CIFS-related properties
will be available in an upcoming release. For information about using the <literal>sharesmb</literal> property, see <olink targetptr="gfwqv" remap="internal">Sharing ZFS Files
in a Solaris CIFS Environment</olink>.</para><para>In addition to the ZFS properties added for supporting
the Solaris CIFS software product, the <literal>vscan</literal> property is
available for scanning ZFS files if you have a 3rd-party virus
scanning engine.</para>
</sect2><sect2 id="gftgp"><title>ZFS Storage Pool Properties</title><para><emphasis role="strong">Solaris Express Developer Edition 1/08:</emphasis> ZFS
storage pool properties were introduced in an earlier release. This release
provides for additional property information. For example:</para><screen># zpool get all users
NAME   PROPERTY     VALUE       SOURCE
users  size         16.8G       -
users  used         217M        -
users  available    16.5G       -
users  capacity     1%          -
users  altroot      -           default
users  health       ONLINE      -
users  guid         11063207170669925585  -
users  version      8           default
users  bootfs       -           default
users  delegation   on          default
users  autoreplace  off         default
users  temporary    on          local</screen><para>For a description of these properties, see <olink targetptr="gfiex" remap="internal">Table&nbsp;4&ndash;1</olink>.</para><itemizedlist><listitem><para>The <literal>cachefile</literal> property &ndash; <emphasis role="strong">Solaris Express Developer Edition 1/08:</emphasis> This
release provides the <literal>cachefile</literal> property, which controls
where pool configuration information is cached. All  pools in the cache are
automatically imported when the system boots. However, installation and clustering
environments might need to cache this information in a different location
so that pools are not automatically imported.</para><para>You can set this
property to cache pool configuration in a different location that can be imported
later by using the <command>zpool import</command> <command>c</command> command.
For most ZFS configurations, this property would not be used.</para><para>The <literal>cachefile</literal> property is not persistent and is not stored on disk.
This property replaces the <literal>temporary</literal> property that was
used to indicate that pool information should not be cached in previous Solaris
releases.</para>
</listitem><listitem><para>The <literal>failmode</literal> property &ndash; <emphasis role="strong">Solaris Express Developer Edition 1/08:</emphasis> This
release provides the <literal>failmode</literal> property for determining
the behavior of a catastrophic pool failure due to a loss of device connectivity
or the failure of all devices in the pool. The <literal>failmode</literal> property
can be set to these values: <literal>wait</literal>, <literal>continue</literal>,
or <literal>panic</literal>. The default value is <literal>wait</literal>,
which means you must reconnect the device or replace a failed device and clear
the error with the <command>zpool clear</command> command.</para><para>The <literal>failmode</literal> property is set like other settable ZFS properties, which
can be set either before or after the pool is created. For example:</para><screen># zpool set failmode=continue tank
# zpool get failmode tank
NAME  PROPERTY  VALUE     SOURCE
tank  failmode  continue  local</screen><screen># zpool create -o failmode=continue</screen><para>For a description of all ZFS pool properties, see <olink targetptr="gfiex" remap="internal">Table&nbsp;4&ndash;1</olink>.</para>
</listitem>
</itemizedlist>
</sect2><sect2 id="gfwqw"><title>ZFS and File System Mirror Mounts</title><para><emphasis role="strong">Solaris Express Developer Edition 1/08:</emphasis> In
this Solaris release, NFSv4 mount enhancements are provided to make ZFS file
systems more accessible to NFS clients.</para><para>When file systems are created on the NFS server, the NFS client can
automatically discover these newly created file systems within their existing
mount of a parent file system.</para><para>For example, if the server <literal>neo</literal> already shares the <literal>tank</literal> file system and client <literal>zee</literal> has it mounted, <filename>/tank/baz</filename> is automatically visible on the client after it is created
on the server. </para><screen>zee# mount neo:/tank /mnt
zee# ls /mnt
baa    bar

neo# zfs create tank/baz

zee% ls /mnt
baa    bar    baz
zee% ls /mnt/baz
file1    file2 </screen>
</sect2><sect2 id="gfiit"><title>ZFS Command History Enhancements (<command>zpool
history</command>)</title><para><emphasis role="strong">Solaris Express Developer Edition 9/07:</emphasis> The <command>zpool history</command> command has been enhanced to provide the following
new features:</para><itemizedlist><listitem><para>ZFS file system event information</para>
</listitem><listitem><para>A <option>l</option> option for displaying a long format that
includes the user  name, the hostname, and the zone in which the operation
was performed </para>
</listitem><listitem><para>A <option>i</option> option for displaying internal event
information that can be used for diagnostic purposes</para>
</listitem>
</itemizedlist><para>For example, the <command>zpool history</command> command provides both <command>zpool</command> command events and <command>zfs</command> command events.</para><screen># zpool history users
History for 'users':
2007-04-26.12:44:02 zpool create users mirror c0t8d0 c0t9d0 c0t10d0
2007-04-26.12:44:38 zfs create users/markm
2007-04-26.12:44:47 zfs create users/marks
2007-04-26.12:44:57 zfs create users/neil
2007-04-26.12:47:15 zfs snapshot -r users/home@yesterday
2007-04-26.12:54:50 zfs snapshot -r users/home@today
2007-04-26.13:29:13 zfs create users/snapshots
2007-04-26.13:30:00 zfs create -o compression=gzip users/snapshots
2007-04-26.13:31:24 zfs create -o compression=gzip-9 users/oldfiles
2007-04-26.13:31:47 zfs set copies=2 users/home
2007-06-25.14:22:52 zpool offline users c0t10d0
2007-06-25.14:52:42 zpool online users c0t10d0
2007-06-25.14:53:06 zpool upgrade users</screen><para>The <command>zpool history</command> <option>i</option> option provides
internal event information. For example:</para><screen remap="wide"># zpool history -i

.
.
.
2007-08-08.15:10:02 [internal create txg:348657] dataset = 83
2007-08-08.15:10:03 zfs create tank/mark
2007-08-08.15:27:41 [internal permission update txg:348869] ul$76928 create dataset = 5
2007-08-08.15:27:41 [internal permission update txg:348869] ul$76928 destroy dataset = 5
2007-08-08.15:27:41 [internal permission update txg:348869] ul$76928 mount dataset = 5
2007-08-08.15:27:41 [internal permission update txg:348869] ud$76928 create dataset = 5
2007-08-08.15:27:41 [internal permission update txg:348869] ud$76928 destroy dataset = 5
2007-08-08.15:27:41 [internal permission update txg:348869] ud$76928 mount dataset = 5
2007-08-08.15:27:41 zfs allow marks create,destroy,mount tank
2007-08-08.15:27:59 [internal permission update txg:348873] ud$76928 snapshot dataset = 5
2007-08-08.15:27:59 zfs allow -d marks snapshot tank</screen><para>The <command>zpool history</command> <option>l</option> option provides
a long format. For example:</para><screen remap="wide"># zpool history -l tank
History for 'tank':
2007-07-19.10:55:13 zpool create tank mirror c0t1d0 c0t11d0 [user root on neo:global]
2007-07-19.10:55:19 zfs create tank/cindys [user root on neo:global]
2007-07-19.10:55:49 zfs allow cindys create,destroy,mount,snapshot tank/cindys [user root on neo:global]
2007-07-19.10:56:24 zfs create tank/cindys/data [user cindys on neo:global]</screen><para>For more information about using the <command>zpool history</command> command,
see <olink targetptr="gbbuw" remap="internal">Identifying Problems in ZFS</olink>.</para>
</sect2><sect2 id="gffys"><title>Upgrading ZFS File Systems (<command>zfs upgrade</command>)</title><para><emphasis role="strong">Solaris Express Developer Edition 9/07:</emphasis> The <command>zfs upgrade</command> command is included in this release to provide future
ZFS file system enhancements to existing file systems. ZFS storage pools have
a similar upgrade feature to provide pool enhancements to existing storage
pools.</para><para>For example:</para><screen># zfs upgrade
This system is currently running ZFS filesystem version 2.

The following filesystems are out of date, and can be upgraded.  After being
upgraded, these filesystems (and any 'zfs send' streams generated from
subsequent snapshots) will no longer be accessible by older software versions.

VER  FILESYSTEM
---  ------------
 1   datab
 1   datab/users
 1   datab/users/area51</screen><note><para>File systems that are upgraded and any streams created from those
upgraded file systems by the <command>zfs send</command> command are not accessible
on systems that are running older software releases.</para>
</note><para>However, no new ZFS file system upgrade features are provided in this
release.</para>
</sect2><sect2 id="gfgak"><title>ZFS Delegated Administration</title><para><emphasis role="strong">Solaris Express Developer Edition 9/07:</emphasis> In
this release, you can delegate fine-grained permissions to perform ZFS administration
tasks to non-privileged users.</para><para>You can use the <command>zfs allow</command> and <command>zfs unallow</command> commands
to grant and remove permissions.</para><para>You can modify the ability to use delegated administration with the
pool's <literal>delegation</literal> property. For example:</para><screen># zpool get delegation users
NAME  PROPERTY    VALUE       SOURCE
users  delegation  on          default
# zpool set delegation=off users
# zpool get delegation users
NAME  PROPERTY    VALUE       SOURCE
users  delegation  off         local</screen><para>By default, the <literal>delegation</literal> property is enabled.</para><para>For more information, see <olink targetptr="gbchv" remap="internal">Chapter&nbsp;8, ZFS
Delegated Administration</olink> and <olink targetdoc="refman1m" targetptr="zfs-1m" remap="external"><citerefentry><refentrytitle>zfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>.</para>
</sect2><sect2 id="gfgaa"><title>Setting Up Separate ZFS Logging Devices</title><para><emphasis role="strong">Solaris Express Developer Edition 9/07:</emphasis> The
ZFS intent log (ZIL) is provided to satisfy POSIX requirements for synchronous
transactions. For example, databases often require their transactions to be
on stable storage devices when returning  from a system call. NFS and other
applications can also use fsync() to ensure data stability. By default,  the
ZIL is allocated  from blocks within the main storage pool. However, better
performance might be possible by using separate intent log devices in your
ZFS storage pool, such as with NVRAM or a dedicated disk.</para><para>Log devices for the ZFS intent log are not related to database log files.</para><para>You can set up a ZFS logging device when the storage pool is created
or after the pool is created. For examples of setting up log devices, see <olink targetptr="gffyt" remap="internal">Creating a ZFS Storage Pool with Log Devices</olink> and <olink targetptr="gazgw" remap="internal">Adding Devices to a Storage Pool</olink>.</para><para>You can attach a log device to an existing log device to create a mirrored
log device. This operation is identical to attaching a device in a unmirrored
storage pool.</para><para>Consider the following points when determining whether setting up a
ZFS log device is appropriate for your environment:</para><itemizedlist><listitem><para>Any performance improvement seen by implementing a separate
log device depends on the device type, the hardware configuration of the pool,
and the application workload. For preliminary performance information, see
this blog:</para><para><ulink url="http://blogs.sun.com/perrin/entry/slog_blog_or_blogging_on" type="url"></ulink></para>
</listitem><listitem><para>Log devices can be unreplicated or mirrored, but RAIDZ is
not supported for log devices.</para>
</listitem><listitem><para>If a separate log device is not mirrored and the device that
contains the log fails, storing log blocks reverts to the storage pool.</para>
</listitem><listitem><para>Log devices can be added, replaced, attached, detached, and
imported and exported as part of the larger storage pool. Currently, log devices
cannot be removed.</para>
</listitem><listitem><para>The minimum size of a log device is the same as the minimum
size of device in pool, which is 64 Mbytes. The amount of in-play data that
might be stored on a log device is relatively small. Log blocks are freed
when the log transaction (system call) is committed.</para>
</listitem><listitem><para>The maximum size of a log device should be approximately 1/2
the size of physical memory because that is the maximum amount of potential
in-play data that can be stored. For example, if a system has 16 Gbytes of
physical memory, consider a maximum log device size of 8 Gbytes.</para>
</listitem>
</itemizedlist>
</sect2><sect2 id="gfgam"><title>Creating Intermediate ZFS Datasets</title><para><emphasis role="strong">Solaris Express Developer Edition 9/07:</emphasis> You
can use the <option>p</option> option with the <command>zfs create</command>, <command>zfs clone</command>, and <command>zfs rename</command> commands to quickly
create a non-existent intermediate dataset, if it doesn't already exist.</para><para>For example, create ZFS datasets (<literal>users/area51</literal>) in
the <literal>datab</literal> storage pool.</para><screen># zfs list
NAME                        USED  AVAIL  REFER  MOUNTPOINT
datab                       106K  16.5G    18K  /datab
# zfs create -p -o compression=on datab/users/area51</screen><para>If the intermediate dataset exists during the create operation, the
operation completes successfully.</para><para>Properties specified apply to the target dataset, not to the intermediate
datasets. For example:</para><screen># zfs get mountpoint,compression datab/users/area51
NAME                PROPERTY     VALUE                SOURCE
datab/users/area51  mountpoint   /datab/users/area51  default
datab/users/area51  compression  on                   local</screen><para>The intermediate dataset is created with the default mount point. Any
additional properties are disabled for the intermediate dataset. For example:</para><screen># zfs get mountpoint,compression datab/users
NAME         PROPERTY     VALUE         SOURCE
datab/users  mountpoint   /datab/users  default
datab/users  compression  off           default</screen><para>For more information, see <olink targetdoc="refman1m" targetptr="zfs-1m" remap="external"><citerefentry><refentrytitle>zfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>.</para>
</sect2><sect2 id="gfgac"><title>ZFS Hotplugging Enhancements</title><para><emphasis role="strong">Solaris Express Developer Edition 9/07:</emphasis> 
In this release, ZFS more effectively responds to devices that are removed
and provides a mechanism to automatically identify devices that are inserted
with the following enhancements:</para><itemizedlist><listitem><para>You can replace an existing device with an equivalent device
without having to use the <command>zpool replace</command> command.</para><para>The <literal>autoreplace</literal> property controls automatic device replacement. If set
to off, device replacement must be initiated by the administrator by using
the <command>zpool replace</command> command. If set to on, any new device,
found in the same physical location as a device that previously belonged to
the pool, is automatically formatted and replaced. The default behavior is
off.</para>
</listitem><listitem><para>The storage pool state <literal>REMOVED</literal> is provided
when a device or hot spare has been removed if the device was physically removed
while the system was running. A hot-spare device is substituted for the removed
device, if available.</para>
</listitem><listitem><para>If a device is removed and then inserted, the device is placed
online. If a hot-spare was activated when the device is re-inserted, the spare
is removed when the online operation completes.</para>
</listitem><listitem><para>Automatic detection when devices are removed or inserted is
hardware-dependent and might not be supported on all platforms. For example,
USB devices are automatically configured upon inserted. However, you might
have to use the <command>cfgadm</command> <option>c configure</option> command
to configure a SATA drive.</para>
</listitem><listitem><para>Hot spares are checked periodically to make sure they are
online and available.</para>
</listitem>
</itemizedlist><para>For more information, see <olink targetdoc="refman1m" targetptr="zpool-1m" remap="external"><citerefentry><refentrytitle>zpool</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>.</para>
</sect2><sect2 id="gevnp"><title>Recursively Renaming ZFS Snapshots (<command>zfs
rename</command> <option>r</option>)</title><para><emphasis role="strong">Solaris Express Developer Edition 5/07:</emphasis> You
can recursively rename all descendent ZFS snapshots by using the <command>zfs
rename</command> <option>r</option> command. </para><para>For example, snapshot a set of ZFS file systems.</para><screen># <userinput>zfs snapshot -r users/home@today</userinput>
# <userinput>zfs list</userinput>
NAME                     USED  AVAIL  REFER  MOUNTPOINT
users                    216K  16.5G    20K  /users
users/home                76K  16.5G    22K  /users/home
users/home@today            0      -    22K  -
users/home/markm          18K  16.5G    18K  /users/home/markm
users/home/markm@today      0      -    18K  -
users/home/marks          18K  16.5G    18K  /users/home/marks
users/home/marks@today      0      -    18K  -
users/home/neil           18K  16.5G    18K  /users/home/neil
users/home/neil@today       0      -    18K  -</screen><para>Then, rename the snapshots the following day.</para><screen># <userinput>zfs rename -r users/home@today @yesterday</userinput>
# <userinput>zfs list</userinput>
NAME                         USED  AVAIL  REFER  MOUNTPOINT
users                        216K  16.5G    20K  /users
users/home                    76K  16.5G    22K  /users/home
users/home@yesterday            0      -    22K  -
users/home/markm              18K  16.5G    18K  /users/home/markm
users/home/markm@yesterday      0      -    18K  -
users/home/marks              18K  16.5G    18K  /users/home/marks
users/home/marks@yesterday      0      -    18K  -
users/home/neil               18K  16.5G    18K  /users/home/neil
users/home/neil@yesterday       0      -    18K  -</screen><para>Snapshots are the only dataset that can be renamed recursively. </para><para>For more information about snapshots, see <olink targetptr="gbciq" remap="internal">Overview
of ZFS Snapshots</olink> and this blog entry that describes how to create
rolling snapshots:</para><para><ulink url="http://blogs.sun.com/mmusante/entry/rolling_snapshots_made_easy" type="url">http://blogs.sun.com/mmusante/entry/rolling_snapshots_made_easy</ulink></para>
</sect2><sect2 id="gevpi"><title>GZIP Compression is Available for ZFS</title><para><emphasis role="strong">Solaris Express Developer Edition 5/07:</emphasis> In
this Solaris release, you can set <literal>gzip</literal> compression on ZFS
file systems in addition to <literal>lzjb</literal> compression. You can specify
compression as <literal>gzip</literal>, the default, or <literal>gzip-</literal><replaceable>N</replaceable>, where <replaceable>N</replaceable> equals 1 through 9. For
example:</para><screen># zfs create -o compression=gzip users/home/snapshots
# zfs get compression users/home/snapshots
NAME                  PROPERTY     VALUE            SOURCE
users/home/snapshots  compression  gzip             local
# zfs create -o compression=gzip-9 users/home/oldfiles
# zfs get compression users/home/oldfiles
NAME                  PROPERTY     VALUE           SOURCE
users/home/oldfiles   compression  gzip-9          local</screen><para>For more information about setting ZFS properties, see <olink targetptr="gazsp" remap="internal">Setting ZFS Properties</olink>.</para>
</sect2><sect2 id="gevpg"><title>Storing Multiple Copies of ZFS User Data</title><para><emphasis role="strong">Solaris Express Developer Edition 5/07:</emphasis> As
a reliability feature, ZFS file system metadata is automatically stored multiple
times across different disks, if possible. This feature is known as <emphasis>ditto
blocks</emphasis>.</para><para>In this Solaris release, you can specify that multiple copies of user
data is also stored per file system by using the <command>zfs set copies</command> command.
For example:</para><screen># <userinput>zfs set copies=2 users/home</userinput>
# <userinput>zfs get copies users/home</userinput>
NAME        PROPERTY  VALUE       SOURCE
users/home  copies    2           local</screen><para>Available values are 1, 2, or 3. The default value is 1. These copies
are in addition to any pool-level redundancy, such as in a mirrored or RAID-Z
configuration.</para><para>The benefits of storing multiple copies of ZFS user data are as follows:</para><itemizedlist><listitem><para>Improves data retention by allowing recovery from unrecoverable
block read faults, such as media faults (bit rot) for all ZFS configurations.</para>
</listitem><listitem><para>Provides data protection even in the case where only a single
disk is available.</para>
</listitem><listitem><para>Allows you to select data protection policies on a per-file
system basis, beyond the capabilities of the storage pool.</para>
</listitem>
</itemizedlist><para>Depending on the allocation of the ditto blocks in the storage pool,
multiple copies might be placed on a single disk. A subsequent full disk failure
might cause all ditto blocks to be unavailable.</para><para>You might consider using ditto blocks when you accidentally create a
non-redundant pool and when you need to set data retention policies.</para><para>For a detailed description of how setting copies on a system with a
single-disk pool or a multiple-disk pool might impact overall data protection,
see this blog:</para><para><ulink url="http://blogs.sun.com/relling/entry/zfs_copies_and_data_protection" type="text">http://blogs.sun.com/relling/entry/zfs_copies_and_data_protection</ulink></para><para>For more information about setting ZFS properties, see <olink targetptr="gazsp" remap="internal">Setting ZFS Properties</olink>.</para>
</sect2><sect2 id="gebws"><title>Improved <command>zpool status</command> Output</title><para><emphasis role="strong">Solaris Express 1/07:</emphasis> You
can use the <command>zpool status</command> <option>v</option> command to
display a list of files with persistent errors. Previously, you had to use
the <command>find</command> <option>inum</option> command to identify the
filenames from the list of displayed inodes.</para><para>For more information about displaying a list of files with persistent
errors, see <olink targetptr="gbctx" remap="internal">Repairing a Corrupted File or Directory</olink>.</para>
</sect2><sect2 id="gebwq"><title>ZFS and Solaris iSCSI Improvements</title><para><emphasis role="strong">Solaris Express, Developer Edition 2/07:</emphasis> In
this Solaris release, you can create a ZFS volume as a Solaris iSCSI target
device by setting the <literal>shareiscsi</literal> property on the ZFS volume.
This method is a convenient way to quickly set up a Solaris iSCSI target.
For example:</para><screen># zfs create -V 2g tank/volumes/v2
# zfs set shareiscsi=on tank/volumes/v2
# iscsitadm list target
Target: tank/volumes/v2
    iSCSI Name: iqn.1986-03.com.sun:02:984fe301-c412-ccc1-cc80-cf9a72aa062a
    Connections: 0</screen><para>After the iSCSI target is created, set up the iSCSI initiator. For information
about setting up a Solaris iSCSI initiator, see <olink targetdoc="sagdfs" targetptr="fmvcd" remap="external">Chapter 14, <citetitle remap="chapter">Configuring Solaris
iSCSI Targets and Initiators (Tasks),</citetitle> in <citetitle remap="book">System
Administration Guide: Devices and File Systems</citetitle></olink>.</para><para>For more information about managing a ZFS volume as an iSCSI target,
see <olink targetptr="gechv" remap="internal">Using a ZFS Volume as a Solaris iSCSI Target</olink>.</para>
</sect2><sect2 id="gebwp"><title>Sharing ZFS File System Enhancements</title><para><emphasis role="strong">Solaris Express, Developer Edition 2/07:</emphasis> In
this Solaris release, the process of sharing file systems has been improved.
Although modifying system configuration files, such as <filename>/etc/dfs/dfstab</filename>,
is unnecessary for sharing ZFS file systems, you can use the <command>sharemgr</command> command
to manage ZFS share properties. The <command>sharemgr</command> command enables
you to set and manage share properties on share groups. ZFS shares are automatically
designated in the <literal>zfs</literal> share group.</para><para>As in previous releases, you can set the ZFS <literal>sharenfs</literal> property
on a ZFS file system to share a ZFS file system. For example:</para><screen># zfs set sharenfs=on tank/home</screen><para>Or, you can use the new <command>sharemgr</command> <command>add-share</command> subcommand
to share a ZFS file system in the <literal>zfs</literal> share group. For
example:</para><screen># sharemgr add-share -s tank/data zfs
# sharemgr show -vp zfs
zfs nfs=()
    zfs/tank/data
          /tank/data
          /tank/data/1
          /tank/data/2
          /tank/data/3</screen><para>Then, you can use the <command>sharemgr</command> command to manage
ZFS shares. The following example shows how to use <literal>sharemgr</literal> to
set the <literal>nosuid</literal> property on the shared ZFS file systems.
You must preface ZFS share paths with <literal>/zfs</literal> designation.</para><screen># sharemgr set -P nfs -p nosuid=true zfs/tank/data
# sharemgr show -vp zfs
zfs nfs=()
    zfs/tank/data nfs=(nosuid="true")
          /tank/data
          /tank/data/1
          /tank/data/2
          /tank/data/3</screen><para>For more information, see <olink targetdoc="refman1m" targetptr="sharemgr-1m" remap="external"><citerefentry><refentrytitle>sharemgr</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>.</para>
</sect2><sect2 id="gdswe"><title>ZFS Command History (<command>zpool history</command>)</title><para><emphasis role="strong">Solaris Express 12/06:</emphasis> In
this Solaris release, ZFS automatically logs successful <command>zfs</command> and <command>zpool</command> commands that modify pool state information. For example:</para><screen remap="wide"># <userinput>zpool history</userinput>
History for 'newpool':
2007-04-25.11:37:31 zpool create newpool mirror c0t8d0 c0t10d0
2007-04-25.11:37:46 zpool replace newpool c0t10d0 c0t9d0
2007-04-25.11:38:04 zpool attach newpool c0t9d0 c0t11d0
2007-04-25.11:38:09 zfs create newpool/user1
2007-04-25.11:38:15 zfs destroy newpool/user1

History for 'tank':
2007-04-25.11:46:28 zpool create tank mirror c1t0d0 c2t0d0 mirror c3t0d0 c4t0d0</screen><para>This features enables you or Sun support personnel to identify the <emphasis>exact</emphasis> set of ZFS commands that was executed to troubleshoot an
error scenario.</para><para>You can identify a specific storage pool with the <command>zpool history</command> command.
For example:</para><screen># <userinput>zpool history newpool</userinput>
History for 'newpool':
History for 'newpool':
2007-04-25.11:37:31 zpool create newpool mirror c0t8d0 c0t10d0
2007-04-25.11:37:46 zpool replace newpool c0t10d0 c0t9d0
2007-04-25.11:38:04 zpool attach newpool c0t9d0 c0t11d0
2007-04-25.11:38:09 zfs create newpool/user1
2007-04-25.11:38:15 zfs destroy newpool/user1</screen><para>The features of the history log are as follows:</para><itemizedlist><listitem><para>The log cannot be disabled.</para>
</listitem><listitem><para>The log is saved persistently on disk, which means the log
is saved across system reboots.</para>
</listitem><listitem><para>The log is implemented as a ring buffer. The minimum size
is 128 Kbytes. The maximum size is 32 Mbytes.</para>
</listitem><listitem><para>For smaller pools, the maximum size is capped at 1% of the
pool size, where <replaceable>size</replaceable> is determined at pool creation
time.</para>
</listitem><listitem><para>Requires no administration, which means tuning the size of
the log or changing the location of the log is unnecessary.</para>
</listitem>
</itemizedlist><para>Currently, the <command>zpool history</command> command does not record
 <replaceable>user-ID</replaceable>, <replaceable>hostname</replaceable>,
or <replaceable>zone-name</replaceable>.</para><para>For more information about troubleshooting ZFS problems, see <olink targetptr="gbbuw" remap="internal">Identifying Problems in ZFS</olink>.</para>
</sect2><sect2 id="gdswf"><title>ZFS Property Improvements</title><sect3 id="gebwr"><title>ZFS <literal>xattr</literal> Property</title><para><emphasis role="strong">Solaris Express 1/07:</emphasis> You
can use the <literal>xattr</literal> property to disable or enable extended
attributes for a specific ZFS file system. The default value is on. For a
description of ZFS properties, see <olink targetptr="gazss" remap="internal">Introducing ZFS
Properties</olink>.</para>
</sect3><sect3 id="gdswd"><title>ZFS <command>canmount</command> Property</title><para><emphasis role="strong">Solaris Express 10/06:</emphasis> The
new <command>canmount</command> property allows you to specify whether a dataset
can be mounted by using the <command>zfs mount</command> command. For more
information, see <olink targetptr="gdrcf" remap="internal">The canmount Property</olink>.</para>
</sect3><sect3 id="gdsus"><title>ZFS User Properties</title><para><emphasis role="strong">Solaris Express 10/06:</emphasis> In
addition to the standard native properties that can either export internal
statistics or control ZFS file system behavior, ZFS supports user properties.
User properties have no effect on ZFS behavior, but you can use them to annotate
datasets with information that is meaningful in your environment.</para><para>For more information, see <olink targetptr="gdrcw" remap="internal">ZFS User Properties</olink>.</para>
</sect3><sect3 id="gdsvx"><title>Setting Properties When Creating ZFS File Systems</title><para><emphasis role="strong">Solaris Express 10/06:</emphasis> In
this Solaris release, you can set properties when you create a file system,
in addition to setting properties after the file system is created.</para><para>The following examples illustrate equivalent syntax:</para><screen># <userinput>zfs create tank/home</userinput>
# <userinput>zfs set mountpoint=/export/zfs tank/home</userinput>
# <userinput>zfs set sharenfs=on tank/home</userinput>
# <userinput>zfs set compression=on tank/home</userinput></screen><screen># <userinput>zfs create -o mountpoint=/export/zfs -o sharenfs=on -o compression=on tank/home</userinput></screen>
</sect3>
</sect2><sect2 id="gdsvh"><title>Displaying All ZFS File System Information</title><para><emphasis role="strong">Solaris Express 10/06:</emphasis> In
this Solaris release, you can use various forms of the <command>zfs get</command> command
to display information about all datasets if you do not specify a dataset.
In previous releases, all dataset information was not retreivable with the <command>zfs get</command> command.</para><para>For example:</para><screen># <userinput>zfs get -s local all</userinput>
tank/home               atime          off                    local
tank/home/bonwick       atime          off                    local
tank/home/marks         quota          50G                    local</screen>
</sect2><sect2 id="gdsup"><title>New <command>zfs receive</command> <option>F</option> Option</title><para><emphasis role="strong">Solaris Express 10/06:</emphasis> In
this Solaris release, you can use the new <option>F</option> option to the <command>zfs receive</command> command to force a rollback of the file system to the
most recent snapshot before doing the receive. Using this option might be
necessary when the file system is modified between the time a rollback occurs
and the receive is initiated.</para><para>For more information, see <olink targetptr="gbimy" remap="internal">Restoring a ZFS Snapshot</olink>.</para>
</sect2><sect2 id="gdfdt"><title>Recursive ZFS Snapshots</title><para><emphasis role="strong">Solaris Express 8/06:</emphasis> When
you use the <command>zfs snapshot</command> command to create a file system
snapshot, you can use the <option>r</option> option to recursively create
snapshots for all descendent file systems. In addition, using the <option>r</option> option
recursively destroys all descendent snapshots when a snapshot is destroyed.</para><para>Recursive ZFS snapshots are created quickly as one atomic operation.
The snapshots are created together (all at once) or not created at all. The
benefit of atomic snapshots operations is that the snapshot data is always
taken at one consistent time, even across descendent file systems.</para><para>For more information, see <olink targetptr="gbcya" remap="internal">Creating and Destroying
ZFS Snapshots</olink>.</para>
</sect2><sect2 id="gcviu"><title>Double Parity RAID-Z (<literal>raidz2</literal>)</title><para><emphasis role="strong">Solaris Express 7/06:</emphasis> A
redundant RAID-Z configuration can now have either single- or double-parity,
which means that one or two device failures can be sustained respectively,
without any data loss. You can specify the <literal>raidz2</literal> keyword
for a double-parity RAID-Z configuration. Or,  you can specify the <literal>raidz</literal> or <literal>raidz1</literal> keyword for a single-parity RAID-Z
configuration.</para><para>For more information, see <olink targetptr="gcvjg" remap="internal">Creating RAID-Z Storage
Pools</olink> or <olink targetdoc="refman1m" targetptr="zpool-1m" remap="external"><citerefentry><refentrytitle>zpool</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>.</para>
</sect2><sect2 id="gcvdm"><title>Hot Spares for ZFS Storage Pool Devices</title><para><emphasis role="strong">Solaris Express 7/06:</emphasis> The
ZFS hot spares feature enables you to identify disks that could be used to
replace a failed or faulted device in one or more storage pools. Designating
a device as a <emphasis>hot spare</emphasis> means that if an active device
in the pool fails, the hot spare automatically replaces the failed device.
Or, you can manually replace a device in a storage pool with a hot spare.</para><para>For more information, see <olink targetptr="gcvcw" remap="internal">Designating Hot Spares
in Your Storage Pool</olink> and <olink targetdoc="refman1m" targetptr="zpool-1m" remap="external"><citerefentry><refentrytitle>zpool</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>.</para>
</sect2><sect2 id="gcvgc"><title>Replacing a ZFS File System With a ZFS Clone (<command>zfs
promote</command>)</title><para><emphasis role="strong">Solaris Express 7/06:</emphasis> The <command>zfs promote</command> command enables you to replace an existing ZFS file
system with a clone of that file system. This feature is helpful when you
want to run tests on an alternative version of a file system and then, make
that alternative version of the file system the active file system.</para><para>For more information, see <olink targetptr="gcvfl" remap="internal">Replacing a ZFS File
System With a ZFS Clone</olink> and <olink targetdoc="refman1m" targetptr="zfs-1m" remap="external"><citerefentry><refentrytitle>zfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>.</para>
</sect2><sect2 id="gcvit"><title>Upgrading ZFS Storage Pools (<command>zpool upgrade</command>)</title><para><emphasis role="strong">Solaris Express 6/06:</emphasis> You
can upgrade your storage pools to a newer version to take advantage of the
latest features by using the <command>zpool upgrade</command> command. In
addition, the <command>zpool status</command> command has been modified to
notify you when your pools are running older versions.</para><para>For more information, see <olink targetptr="gcikw" remap="internal">Upgrading ZFS Storage
Pools</olink> and <olink targetdoc="refman1m" targetptr="zpool-1m" remap="external"><citerefentry><refentrytitle>zpool</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>.</para><para>If you want to use the ZFS Administration console on a system with a
pool from a previous Solaris release, make sure you upgrade your pools before
using the ZFS Administration console. To see if your pools need to be upgraded,
use the <command>zpool status</command> command. For information about the
ZFS Administration console, see <olink targetptr="gbsbp" remap="internal">ZFS Web-Based Management</olink>.</para>
</sect2><sect2 id="gcsxk"><title>Using ZFS to Clone Non-Global Zones and Other Enhancements</title><para><emphasis role="strong">Solaris Express 6/06:</emphasis> When
the source <literal>zonepath</literal> and the target <literal>zonepath</literal> both
reside on ZFS and are in the same pool, <command>zoneadm clone</command> now
automatically uses the ZFS  clone feature to clone a zone. This enhancement
means that <command>zoneadm clone</command> will take a ZFS snapshot of the
source <literal>zonepath</literal> and set up the target <literal>zonepath</literal>.
The snapshot is named <literal>SUNWzoneX</literal>, where <literal>X</literal> is
a unique ID used to distinguish between multiple snapshots. The destination
zone's <literal>zonepath</literal> is used to name the ZFS clone. A software
inventory is performed so that a snapshot used at a future time can be validated
by the system. Note that you can still specify that the ZFS <literal>zonepath</literal> be
copied instead of the ZFS clone, if desired.</para><para>To clone a source zone multiple times, a new parameter added to <command>zoneadm</command> allows you to specify that an existing snapshot should be used.
The system validates that the existing snapshot is usable on the target. Additionally,
the zone install process now has the capability to detect when a ZFS file
system can be created for a zone, and the uninstall process can detect when
a ZFS file system in a zone can be destroyed. These steps are then performed
automatically by the <command>zoneadm</command> command.</para><para>Keep the following points in mind when using ZFS on a system with Solaris
containers installed:</para><itemizedlist><listitem><para>Do not use the ZFS snapshot features to clone a zone</para>
</listitem><listitem><para>You can delegate or a add a ZFS file system to a non-global
zone. For more information, see <olink targetptr="gbbrq" remap="internal">Adding ZFS File Systems
to a Non-Global Zone</olink> or <olink targetptr="gbbst" remap="internal">Delegating Datasets
to a Non-Global Zone</olink>.</para>
</listitem><listitem><para>Do not use a ZFS file system for a global zone root path or
a non-global zone root path in the Solaris 10 releases. You can use ZFS as
a zone root path in the Solaris Express releases, but keep in mind that patching
or upgrading these zones is not supported.</para>
</listitem>
</itemizedlist><para>For more information, see <olink targetdoc="sysadrm" remap="external"><citetitle remap="book">System Administration Guide:  Virtualization Using the Solaris
Operating System</citetitle></olink>.</para>
</sect2><sect2 id="gciui"><title>ZFS Backup and Restore Commands are Renamed</title><para><emphasis role="strong">Solaris Express 5/06:</emphasis> In
this Solaris release, the <command>zfs backup</command> and <command>zfs restore</command> commands
are renamed to <command>zfs send</command> and <command>zfs receive</command> to
more accurately describe their function. The function of these commands is
to save and restore ZFS data stream representations.</para><para>For more information about these commands, see <olink targetptr="gbchx" remap="internal">Saving
and Restoring ZFS Data</olink>.</para>
</sect2><sect2 id="gcitn"><title>Recovering Destroyed Storage Pools</title><para><emphasis role="strong">Solaris Express 5/06:</emphasis> This
release includes the <command>zpool import</command> <option>D</option> command,
which enables you to recover pools that were previously destroyed with the <command>zpool destroy</command> command.</para><para>For more information, see <olink targetptr="gcfhw" remap="internal">Recovering Destroyed
ZFS Storage Pools</olink>.</para>
</sect2><sect2 id="gcfhy"><title>ZFS is Integrated With Fault Manager</title><para><emphasis role="strong">Solaris Express 4/06:</emphasis> This
release includes the integration of a ZFS diagnostic engine that is capable
of diagnosing and reporting pool failures and device failures. Checksum, I/O,
device, and pool errors associated with pool or device failures are also reported.</para><para>The diagnostic engine does not include predictive analysis of  checksum
and I/O errors, nor does it include proactive actions based on fault analysis.</para><para>In the event of the ZFS failure, you might see a message similar to
the following from <command>fmd</command>:</para><screen>SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major
EVENT-TIME: Fri Mar 10 11:09:06 MST 2006
PLATFORM: SUNW,Ultra-60, CSN: -, HOSTNAME: neo
SOURCE: zfs-diagnosis, REV: 1.0
EVENT-ID: b55ee13b-cd74-4dff-8aff-ad575c372ef8
DESC: A ZFS device failed.  Refer to http://sun.com/msg/ZFS-8000-D3 for more information.
AUTO-RESPONSE: No automated response will occur.
IMPACT: Fault tolerance of the pool may be compromised.
REC-ACTION: Run 'zpool status -x' and replace the bad device.</screen><para>By reviewing the recommended action, which will be to follow the more
specific directions in the <command>zpool status</command> command, you will
be able to quickly identify and resolve the failure. </para><para>For an example of recovering from a reported ZFS problem, see <olink targetptr="gbbvb" remap="internal">Repairing a Missing Device</olink>.</para>
</sect2><sect2 id="gcfiw"><title>New <command>zpool clear</command> Command</title><para><emphasis role="strong">Solaris Express 4/06:</emphasis> This
release includes the <command>zpool clear</command> command for clearing error
counts associated with a device or the pool. Previously, error counts were
cleared when a device in a pool was brought online with the <command>zpool
online</command> command. For more information, see <olink targetdoc="refman1m" targetptr="zpool-1m" remap="external"><citerefentry><refentrytitle>zpool</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> and <olink targetptr="gazge" remap="internal">Clearing
Storage Pool Devices</olink>.</para>
</sect2><sect2 id="gcajn"><title>Compact NFSv4 ACL Format</title><para><emphasis role="strong">Solaris Express 4/06:</emphasis> In
this release, three NFSv4 ACL formats are available: verbose, positional,
and compact. The new compact and positional ACL formats are available to set
and display ACLs. You can use the <command>chmod</command> command to set
all 3 ACL formats. You can use the <command>ls</command> <option>V</option> command
to display compact and positional ACL formats and the <command>ls</command> <option>v</option> command to display verbose ACL formats.</para><para>For more information, see <olink targetptr="gbchf" remap="internal">Setting and Displaying
ACLs on ZFS Files in Compact Format</olink>, <olink targetdoc="refman1" targetptr="chmod-1" remap="external"><citerefentry><refentrytitle>chmod</refentrytitle><manvolnum>1</manvolnum></citerefentry></olink>, and <olink targetdoc="refman1" targetptr="ls-1" remap="external"><citerefentry><refentrytitle>ls</refentrytitle><manvolnum>1</manvolnum></citerefentry></olink>.</para>
</sect2><sect2 id="gcakl"><title>File System Monitoring Tool (<command>fsstat</command>)</title><para><emphasis role="strong">Solaris Express 4/06:</emphasis> A
new file system monitoring tool, <command>fsstat</command>, is available to
report file system operations. Activity can be reported by mount point or
by file system type.  The following example shows general ZFS file system
activity.</para><screen>$ <userinput>fsstat zfs</userinput>
 new  name   name  attr  attr lookup rddir  read read  write write
 file remov  chng   get   set    ops   ops   ops bytes   ops bytes
7.82M 5.92M 2.76M 1.02G 3.32M  5.60G 87.0M  363M 1.86T 20.9M  251G zfs</screen><para>For more information, see <olink targetdoc="refman1m" targetptr="fsstat-1m" remap="external"><citerefentry><refentrytitle>fsstat</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>.</para>
</sect2><sect2 id="gbsbp"><title>ZFS Web-Based Management</title><para><emphasis role="strong">Solaris Express 1/06:</emphasis> A
web-based ZFS management tool is available to perform many administrative
actions. With this tool, you can perform the following tasks:</para><itemizedlist><listitem><para>Create a new storage pool.</para>
</listitem><listitem><para>Add capacity to an existing pool.</para>
</listitem><listitem><para>Move (export) a storage pool to another system.</para>
</listitem><listitem><para>Import a previously exported storage pool to make it available
on another system.</para>
</listitem><listitem><para>View information about storage pools.</para>
</listitem><listitem><para>Create a file system.</para>
</listitem><listitem><para>Create a volume.</para>
</listitem><listitem><para>Take a snapshot of a file system or a volume.</para>
</listitem><listitem><para>Roll back a file system to a previous snapshot.</para>
</listitem>
</itemizedlist><para>You can access the ZFS Administration console through a secure web browser
at the following URL:</para><screen>https://<replaceable>system-name</replaceable>:6789/zfs</screen><para>If you type the appropriate URL and are unable to reach the ZFS Administration
console, the server might not be started. To start the server, run the following
command:</para><screen># /usr/sbin/smcwebserver start</screen><para>If you want the server to run automatically when the system boots, run
the following command:</para><screen># /usr/sbin/smcwebserver enable</screen><note><para>You cannot use the Solaris Management Console (<command>smc</command>)
to manage ZFS storage pools or file systems.</para>
</note><para>You
will not be able to manage ZFS file systems remotely with the ZFS Administration
console because of a change in a recent Solaris release, which shutdown some
network services automatically. Use the following command to enable these
services:</para><screen># <userinput>netservices open</userinput></screen>
</sect2>
</sect1><sect1 id="zfsover-2"><title>What Is ZFS?</title><para>The ZFS file system is a revolutionary new file
system that fundamentally changes the way file systems are administered, with
features and benefits not found in any other file system available today.
ZFS has been designed to be robust, scalable, and simple to administer.</para><sect2 id="gaypk"><title>ZFS Pooled Storage</title><para>ZFS uses the concept of <emphasis>storage pools</emphasis> to manage
physical storage. Historically, file systems were constructed on top of a
single physical device. To address multiple devices and provide for data redundancy,
the concept of a <emphasis>volume manager</emphasis> was introduced to provide
the image of a single device so that file systems would not have to be modified
to take advantage of multiple devices. This design added another layer of
complexity and ultimately prevented certain file system advances, because
the file system had no control over the physical placement of data on the
virtualized volumes.</para><para>ZFS eliminates the volume management altogether. Instead of forcing
you to create virtualized volumes, ZFS aggregates devices into a storage pool.
The storage pool describes the physical characteristics of the storage (device
layout, data redundancy, and so on,) and acts as an arbitrary data store from
which file systems can be created. File systems are no longer constrained
to individual devices, allowing them to share space with all file systems
in the pool. You no longer need to predetermine the size of a file system,
as file systems grow automatically within the space allocated to the storage
pool. When new storage is added, all file systems within the pool can immediately
use the additional space without additional work. In many ways, the storage
pool acts as a virtual memory system. When a memory DIMM is added to a system,
the operating system doesn't force you to invoke some commands to configure
the memory and assign it to individual processes. All processes on the system
automatically use the additional memory.</para>
</sect2><sect2 id="gaypi"><title>Transactional Semantics</title><para>ZFS is a transactional file system, which means that the file system
state is always consistent on disk. Traditional file systems overwrite data
in place, which means that if the machine loses power, for example, between
the time a data block is allocated and when it is linked into a directory,
the file system will be left in an inconsistent state. Historically, this
problem was solved through the use of the <command>fsck</command> command.
This command was responsible for going through and verifying file system state,
making an attempt to repair any inconsistencies in the process. This problem
caused great pain to administrators and was never guaranteed to fix all possible
problems. More recently, file systems have introduced the concept of <emphasis>journaling</emphasis>. The journaling process records action in a separate journal,
which can then be replayed safely if a system crash occurs. This process introduces
unnecessary overhead, because the data needs to be written twice, and often
results in a new set of problems, such as when the journal can't be replayed
properly.</para><para>With a transactional file system, data is managed using <emphasis>copy
on write</emphasis> semantics. Data is never overwritten, and any sequence
of operations is either entirely committed or entirely ignored. This mechanism
means that the file system can never be corrupted through accidental loss
of power or a system crash. So, no need for a <command>fsck</command> equivalent
exists. While the most recently written pieces of data might be lost, the
file system itself will always be consistent. In addition, synchronous data
(written using the <varname>O_DSYNC</varname> flag) is always guaranteed to
be written before returning, so it is never lost.</para>
</sect2><sect2 id="gaypb"><title>Checksums and Self-Healing Data</title><para>With ZFS, all data and metadata is checksummed using a user-selectable
algorithm. Traditional file systems that do provide checksumming have performed
it on a per-block basis, out of necessity due to the volume management layer
and traditional file system design. The traditional design means that certain
failure modes, such as writing a complete block to an incorrect location,
can result in properly checksummed data that is actually incorrect. ZFS checksums
are stored in a way such that these failure modes are detected and can be
recovered from gracefully. All checksumming and data recovery is done at the
file system layer, and is transparent to applications.</para><para>In addition, ZFS provides for self-healing data. ZFS supports storage
pools with varying levels of data redundancy, including mirroring and a variation
on RAID-5. When a bad data block is detected, ZFS fetches the correct data
from another redundant copy, and repairs the bad data, replacing it with the
good copy.</para>
</sect2><sect2 id="gayou"><title>Unparalleled Scalability</title><para>ZFS has been designed from the ground up to be the most scalable file
system, ever. The file system itself is 128-bit, allowing for 256 quadrillion
zettabytes of storage. All metadata is allocated dynamically, so no need exists
to pre-allocate inodes or otherwise limit the scalability of the file system
when it is first created. All the algorithms have been written with scalability
in mind. Directories can have up to 2<superscript>48</superscript> (256 trillion)
entries, and no limit exists on the number of file systems or number of files
that can be contained within a file system.</para>
</sect2><sect2 id="gbcbn"><title>ZFS Snapshots</title><para>A <emphasis>snapshot</emphasis> is a read-only copy of a file system
or volume. Snapshots can be created quickly and easily. Initially, snapshots
consume no additional space within the pool.</para><para>As data within the active dataset changes, the snapshot consumes space
by continuing to reference the old data. As a result, the snapshot prevents
the data from being freed back to the pool.</para>
</sect2><sect2 id="gayoc"><title>Simplified Administration</title><para>Most importantly, ZFS provides a greatly simplified administration model.
Through the use of hierarchical file system layout, property inheritance,
and automanagement of mount points and NFS share semantics, ZFS makes it easy
to create and manage file systems without needing multiple commands or editing
configuration files. You can easily set quotas or reservations, turn compression
on or off, or manage mount points for numerous file systems with a single
command. Devices can be examined or repaired without having to understand
a separate set of volume manager commands. You can take an unlimited number
of instantaneous snapshots of file systems. You can backup and restore individual
file systems.</para><para>ZFS manages file systems through a hierarchy that allows for this simplified
management of properties such as quotas, reservations, compression, and mount
points. In this model, file systems become the central point of control. File
systems themselves are very cheap (equivalent to a new directory), so you
are encouraged to create a file system for each user, project, workspace,
and so on. This design allows you to define fine-grained management points.</para>
</sect2>
</sect1><sect1 id="ftyue"><title>ZFS Terminology</title><para>This section describes the basic terminology used throughout this book:</para><variablelist><varlistentry><term>checksum</term><listitem><para>A 256-bit hash of the data in a file system block. The checksum
capability can range from the simple and fast fletcher2 (the default) to cryptographically
strong hashes such as SHA256.</para>
</listitem>
</varlistentry><varlistentry><term>clone</term><listitem><para>A file system whose initial contents are identical to the
contents of a snapshot.</para><para>For information about clones, see <olink targetptr="gbcxz" remap="internal">Overview
of ZFS Clones</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>dataset</term><listitem><para>A generic name for the following ZFS entities: clones, file
systems, snapshots, or volumes.</para><para>Each dataset is identified by a unique name in the ZFS namespace. Datasets
are identified using the following format:</para><para><replaceable>pool</replaceable>/<replaceable>path</replaceable>[<replaceable>@snapshot</replaceable>]</para><variablelist><varlistentry><term><replaceable>pool</replaceable></term><listitem><para>Identifies the name of the storage pool that contains the
dataset</para>
</listitem>
</varlistentry><varlistentry><term><replaceable>path</replaceable></term><listitem><para>Is a slash-delimited path name for the dataset object</para>
</listitem>
</varlistentry><varlistentry><term><replaceable>snapshot</replaceable></term><listitem><para>Is an optional component that identifies a snapshot of a dataset</para>
</listitem>
</varlistentry>
</variablelist><para>For more information about datasets, see <olink targetptr="gavwq" remap="internal">Chapter&nbsp;5,
Managing ZFS File Systems</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>file system</term><listitem><para>A dataset that contains a standard POSIX file system.</para><para>For more information about file systems, see <olink targetptr="gavwq" remap="internal">Chapter&nbsp;5,
Managing ZFS File Systems</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>mirror</term><listitem><para>A virtual device that stores identical copies of data on two
or more disks. If any disk in a mirror fails, any other disk in that mirror
can provide the same data.</para>
</listitem>
</varlistentry><varlistentry><term>pool</term><listitem><para>A logical group of devices describing the layout and physical
characteristics of the available storage. Space for datasets is allocated
from a pool.</para><para>For more information about storage pools, see <olink targetptr="gavwn" remap="internal">Chapter&nbsp;4,
Managing ZFS Storage Pools</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>RAID-Z</term><listitem><para>A virtual device that stores data and parity on multiple disks,
similar to RAID-5. For more information about
RAID-Z, see <olink targetptr="gamtu" remap="internal">RAID-Z Storage Pool Configuration</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>resilvering</term><listitem><para>The process of transferring data from one device to another
device is known as <emphasis>resilvering</emphasis>. For example, if a mirror
component is replaced or taken offline, the data from the up-to-date mirror
component is copied to the newly restored mirror component. This process is
referred to as <emphasis>mirror resynchronization</emphasis> in traditional
volume management products.</para><para>For more information about ZFS resilvering, see <olink targetptr="gbcus" remap="internal">Viewing
Resilvering Status</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>snapshot</term><listitem><para>A read-only image of a file system or volume at a given point
in time.</para><para>For more information about snapshots, see <olink targetptr="gbciq" remap="internal">Overview
of ZFS Snapshots</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>virtual device</term><listitem><para>A logical device in a pool, which can be a physical device,
a file, or a collection of devices.</para><para>For more information about virtual devices, see <olink targetptr="gazca" remap="internal">Identifying
Virtual Devices in a Storage Pool</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>volume</term><listitem><para>A dataset used to emulate a physical device. For example,
you can create an ZFS volume as a swap device.</para><para>For more information about ZFS volumes, see <olink targetptr="gaypf" remap="internal">ZFS
Volumes</olink>.</para>
</listitem>
</varlistentry>
</variablelist>
</sect1><sect1 id="gbcpt"><title>ZFS Component Naming Requirements</title><para>Each ZFS component must be named according to the following rules:</para><itemizedlist><listitem><para>Empty components are not allowed.</para>
</listitem><listitem><para>Each component can only contain alphanumeric characters in
addition to the following four special characters:</para><itemizedlist><listitem><para>Underscore (_)</para>
</listitem><listitem><para>Hyphen (-)</para>
</listitem><listitem><para>Colon (:)</para>
</listitem><listitem><para>Period (.)</para>
</listitem>
</itemizedlist>
</listitem><listitem><para>Pool names must begin with a letter, except for the following
restrictions:</para><itemizedlist><listitem><para>The beginning sequence <literal>c</literal>[<literal>0-9</literal>]
is not allowed</para>
</listitem><listitem><para>The name <literal>log</literal> is reserved</para>
</listitem><listitem><para>A name that begins with <literal>mirror</literal>, <literal>raidz</literal>, or <literal>spare</literal> is not allowed because these name
are reserved.</para>
</listitem>
</itemizedlist><para>In addition, pool names must not contain a percent sign (<literal>%</literal>)</para>
</listitem><listitem><para>Dataset names must begin with an alphanumeric character. Dataset
names must not contain
a percent sign (<literal>%</literal>).</para>
</listitem>
</itemizedlist>
</sect1>
</chapter>
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-type" content="text/html; charset=iso-8859-1">
<title>Creating and Destroying ZFS Storage Pools - Solaris ZFS Administration Guide</title>
<meta name="robots" content="index,follow">
<meta name="robots" content="index,follow">
<meta name="date" content="2008-01-01">
<meta name="collection" content="reference">
<link rel="stylesheet" type="text/css" href="css/elements.css">
<link rel="stylesheet" type="text/css" href="css/indiana.css">
</head>

<body>


<div class="Masthead">
   <div class="MastheadLogo"></div>
   <div class="Title">Solaris ZFS Administration Guide</div>
</div>

<table class="Layout" border="0" cellspacing="0" width="100%">
<tbody>

   <tr valign="top" class="PageControls">
      <td></td>
      <td>
         <table width="100%">
      	   <tr>
      	     <td>
                 <a href="gcfof.html">Previous</a>
             </td>
             <td align="right">
                 <a href="gayrd.html">Next</a>
             </td>
           </tr>
         </table>
      </td>
   </tr>
   
   <tr valign="top">
      <td class="Navigation" width="200px"><p class="toc level1"><a href="docinfo.html">Document Information</a></p>
<p class="toc level1 tocsp"><a href="preface-1.html">Preface</a></p>
<p class="toc level1 tocsp"><a href="zfsover-1.html">1.&nbsp;&nbsp;Solaris ZFS File System (Introduction)</a></p>
<p class="toc level1 tocsp"><a href="setup-1.html">2.&nbsp;&nbsp;Getting Started With ZFS</a></p>
<p class="toc level1 tocsp"><a href="gbcik.html">3.&nbsp;&nbsp;ZFS and Traditional File System Differences</a></p>
<p class="toc level1 tocsp"><a href="gavwn.html">4.&nbsp;&nbsp;Managing ZFS Storage Pools</a></p>
<p class="toc level2"><a href="gcfog.html">Components of a ZFS Storage Pool</a></p>
<p class="toc level2"><a href="gcfof.html">Replication Features of a ZFS Storage Pool</a></p>
<div class="onpage">
<p class="toc level2"><a href="">Creating and Destroying ZFS Storage Pools</a></p>
</div>
<p class="toc level2"><a href="gayrd.html">Managing Devices in ZFS Storage Pools</a></p>
<p class="toc level2"><a href="gfifk.html">Managing ZFS Storage Pool Properties</a></p>
<p class="toc level2"><a href="gaynp.html">Querying ZFS Storage Pool Status</a></p>
<p class="toc level2"><a href="gbchy.html">Migrating ZFS Storage Pools</a></p>
<p class="toc level1 tocsp"><a href="gavwq.html">5.&nbsp;&nbsp;Managing ZFS File Systems</a></p>
<p class="toc level1 tocsp"><a href="gavvx.html">6.&nbsp;&nbsp;Working With ZFS Snapshots and Clones</a></p>
<p class="toc level1 tocsp"><a href="ftyxi.html">7.&nbsp;&nbsp;Using ACLs to Protect ZFS Files</a></p>
<p class="toc level1 tocsp"><a href="gbchv.html">8.&nbsp;&nbsp;ZFS Delegated Administration</a></p>
<p class="toc level1 tocsp"><a href="ftyxh.html">9.&nbsp;&nbsp;ZFS Advanced Topics</a></p>
<p class="toc level1 tocsp"><a href="gavwg.html">10.&nbsp;&nbsp;ZFS Troubleshooting and Data Recovery</a></p>
<p class="toc level1 tocsp"><a href="idx-1.html">Index</a></p>
</td>
      <td class="ContentPane" width="705px">

	 <div class="MainContent">      	 
             

<a name="gaypw"></a><h3>Creating and Destroying ZFS Storage Pools</h3>
<p>The following sections describe different scenarios for creating and destroying ZFS storage pools.</p>
<ul><li><p><a href="#gaynr">Creating a ZFS Storage Pool</a></p></li>
<li><p><a href="#gazhs">Handling ZFS Storage Pool Creation Errors</a></p></li>
<li><p><a href="#gammr">Destroying ZFS Storage Pools</a></p></li></ul>
<p>By design, creating and destroying pools is fast and easy. However, be cautious
when doing these operations. Although checks are performed to prevent using devices known
to be in use in a new pool, ZFS cannot always know when
a device is already in use. Destroying a pool is even easier.
Use <tt>zpool destroy</tt> with caution. This is a simple command with significant consequences. <a name="indexterm-114"></a><a name="indexterm-115"></a></p>

<a name="gaynr"></a><h4>Creating a ZFS Storage Pool</h4>
<p>To create a storage pool, use the <tt>zpool create</tt> command. This command takes a
pool name and any number of virtual devices as arguments. The pool name
must satisfy the naming conventions outlined in <a href="gbcpt.html">ZFS Component Naming Requirements</a>.<a name="indexterm-116"></a><a name="indexterm-117"></a><a name="indexterm-118"></a></p>

<a name="gazgt"></a><h5>Creating a Basic Storage Pool</h5>
<p>The following command creates a new pool named <tt>tank</tt> that consists of the
disks <tt>c1t0d0</tt> and <tt>c1t1d0</tt>:</p><pre># <tt><b>zpool create tank c1t0d0 c1t1d0</b></tt></pre><p>These whole disks are found in the <tt>/dev/dsk</tt> directory and are labelled appropriately
by ZFS to contain a single, large slice. Data is dynamically striped across
both disks.</p>

<a name="gazhv"></a><h5>Creating a Mirrored Storage Pool</h5>
<p>To create a mirrored pool, use the <tt>mirror</tt> keyword, followed by any number
of storage devices that will comprise the mirror. Multiple mirrors can be specified
by repeating the <tt>mirror</tt> keyword on the command line. The following command creates
a pool with two, two-way mirrors:</p><pre># <tt><b>zpool create tank mirror c1d0 c2d0 mirror c3d0 c4d0</b></tt></pre><p>The second <tt>mirror</tt> keyword indicates that a new top-level virtual device is being
specified. Data is dynamically striped across both mirrors, with data being redundant between
each disk appropriately.<a name="indexterm-119"></a><a name="indexterm-120"></a><a name="indexterm-121"></a><a name="indexterm-122"></a></p><p>Currently, the following operations are supported on a ZFS mirrored configuration:</p>
<ul><li><p>Adding another set of disks for an additional top-level <tt>vdev</tt> to an existing mirrored configuration. For more information, see <a href="gayrd.html#gazgw">Adding Devices to a Storage Pool</a>.</p></li>
<li><p>Attaching additional disks to an existing mirrored configuration. Or, attaching additional disks to a non-replicated configuration to create a mirrored configuration. For more information, see <a href="gayrd.html#gcfhe">Attaching and Detaching Devices in a Storage Pool</a>.</p></li>
<li><p>Replace a disk or disks in an existing mirrored configuration as long as the replacement disks are greater than or equal to the device to be replaced. For more information, see <a href="gayrd.html#gazgd">Replacing Devices in a Storage Pool</a>.</p></li>
<li><p>Detach a disk or disk in a mirrored configuration as long as the remaining devices provide adequate redundancy for the configuration. For more information, see <a href="gayrd.html#gcfhe">Attaching and Detaching Devices in a Storage Pool</a>.</p></li></ul>
<p>Currently, the following operations are not supported on a mirrored configuration:</p>
<ul><li><p>You cannot outright remove a device from a mirrored storage pool. An RFE is filed for this feature.</p></li>
<li><p>You cannot split or break a mirror for backup purposes. An RFE is filed for this feature.</p></li></ul>


<a name="gcvjg"></a><h5>Creating RAID-Z Storage Pools</h5>
<p>Creating a single-parity RAID-Z pool is identical to creating a mirrored pool, except
that the <tt>raidz</tt> or <tt>raidz1</tt> keyword is used instead of <tt>mirror</tt>. The following
example shows how to create a pool with a single RAID-Z device that
consists of five disks:<a name="indexterm-123"></a><a name="indexterm-124"></a><a name="indexterm-125"></a><a name="indexterm-126"></a></p><pre># <tt><b>zpool create tank raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 /dev/dsk/c5t0d0</b></tt></pre><p>This example demonstrates that disks can be specified by using their full paths.
The <tt>/dev/dsk/c5t0d0</tt> device is identical to the <tt>c5t0d0</tt> device.</p><p>A similar configuration could be created with disk slices. For example:</p><pre># <tt><b>zpool create tank raidz c1t0d0s0 c2t0d0s0 c3t0d0s0 c4t0d0s0 c5t0d0s0</b></tt></pre><p>However, the disks must be preformatted to have an appropriately sized slice zero.</p><p>You can create a double-parity RAID-Z configuration by using the <tt>raidz2</tt> keyword
when the pool is created. For example:<a name="indexterm-127"></a></p><pre># <tt><b>zpool create tank raidz2 c1t0d0 c2t0d0 c3t0d0</b></tt>
# <tt><b>zpool status -v tank</b></tt>
  pool: tank
 state: ONLINE
 scrub: none requested
config:

        NAME          STATE     READ WRITE CKSUM
        tank          ONLINE       0     0     0
          raidz2      ONLINE       0     0     0
            c1t0d0    ONLINE       0     0     0
            c2t0d0    ONLINE       0     0     0
            c3t0d0    ONLINE       0     0     0

errors: No known data errors</pre><p>Currently, the following operations are supported on a ZFS RAID-Z configuration:</p>
<ul><li><p>Add another set of disks for an additional top-level <tt>vdev</tt> to an existing RAID-Z configuration. For more information, see <a href="gayrd.html#gazgw">Adding Devices to a Storage Pool</a>.</p></li>
<li><p>Replace a disk or disks in an existing RAID-Z configuration as long as the replacement disks are greater than or equal to the device to be replaced. For more information, see <a href="gayrd.html#gazgd">Replacing Devices in a Storage Pool</a>.</p></li></ul>
<p>Currently, the following operations are not supported on a RAID-Z configuration:</p>
<ul><li><p>Attach an additional disk to an existing RAID-Z configuration.</p></li>
<li><p>Detach a disk from a RAID-Z configuration.</p></li>
<li><p>You cannot outright remove a device from a RAID-Z configuration. An RFE is filed for this feature.</p></li></ul>
<p>For more information about a RAID-Z configuration, see <a href="gcfof.html#gamtu">RAID-Z Storage Pool Configuration</a>.</p>

<a name="gffyt"></a><h5>Creating a ZFS Storage Pool with Log Devices</h5>
<p>By default,  the ZIL is allocated  from blocks within the
main pool. However, better performance might be possible by using separate intent log devices,
such as NVRAM or a dedicated disk. For more information about ZFS log
devices, see <a href="gbscy.html#gfgaa">Setting Up Separate ZFS Logging Devices</a>.<a name="indexterm-128"></a><a name="indexterm-129"></a></p><p>You can set up a ZFS logging device when the storage pool
is created or after the pool is created.</p><p>For example, create a mirrored storage pool with mirrored log devices.</p><pre># zpool create datap mirror c1t1d0 c1t2d0 mirror c1t3d0 c1t4d0 log mirror c1t5d0 c1t8d0
# zpool status
  pool: datap
 state: ONLINE
 scrub: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        datap       ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c1t1d0  ONLINE       0     0     0
            c1t2d0  ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c1t3d0  ONLINE       0     0     0
            c1t4d0  ONLINE       0     0     0
        logs        ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c1t5d0  ONLINE       0     0     0
            c1t8d0  ONLINE       0     0     0

errors: No known data errors</pre>

<a name="gfxtd"></a><h5>Creating a ZFS Storage Pool with Cache Devices</h5>
<p>You can create a storage pool with cache devices to cache storage
pool data. For example:<a name="indexterm-130"></a><a name="indexterm-131"></a><a name="indexterm-132"></a></p><pre># zpool create tank mirror c2t0d0 c2t1d0 c2t3d0 cache c2t5d0 c2t8d0
# zpool status tank
  pool: tank
 state: ONLINE
 scrub: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        tank        ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c2t0d0  ONLINE       0     0     0
            c2t1d0  ONLINE       0     0     0
            c2t3d0  ONLINE       0     0     0
        cache
          c2t5d0    ONLINE       0     0     0
          c2t8d0    ONLINE       0     0     0</pre><p>Review the following points when considering whether to create a ZFS storage pool
with cache devices:</p>
<ul><li><p>Using cache devices provide the greatest performance improvement for random read-workloads of mostly static content.</p></li>
<li><p>Capacity and reads can be monitored by using the <tt>zpool iostat</tt> command.</p></li>
<li><p>Single or multiple cache devices can be added when the pool is created or added and removed after the pool is created. For more information, see <a href="gayrd.html#gfxrx">Example&nbsp;4-3</a>.</p></li>
<li><p>Cache devices cannot be mirrored or be part of a RAID-Z configuration.</p></li>
<li><p>If a read error is encountered on a cache device, that read I/O is reissued to the original storage pool device, which might be part of a mirrored or RAID-Z configuration. The content of the cache devices is considered volatile, as is the case with other system caches.</p></li></ul>


<a name="gazhs"></a><h4>Handling ZFS Storage Pool Creation Errors</h4>
<p>Pool creation errors can occur for many reasons. Some of these reasons are
obvious, such as when a specified device doesn't exist, while other reasons are
more subtle.</p>

<a name="gazht"></a><h5>Detecting in Use Devices</h5>
<p>Before formatting a device, ZFS first determines if the disk is in
use by ZFS or some other part of the operating system. If the
disk is in use, you might see errors such as the following:</p><pre># <tt><b>zpool create tank c1t0d0 c1t1d0</b></tt>
invalid vdev specification
use '-f' to override the following errors:
/dev/dsk/c1t0d0s0 is currently mounted on /. Please see umount(1M).
/dev/dsk/c1t0d0s1 is currently mounted on swap. Please see swap(1M).
/dev/dsk/c1t1d0s0 is part of active ZFS pool zeepool. Please see zpool(1M).</pre><p>Some of these errors can be overridden by using the <tt>-f</tt> option, but
most errors cannot. The following uses cannot be overridden by using the
<tt>-f</tt> option, and you must manually correct them:<a name="indexterm-133"></a><a name="indexterm-134"></a></p><dl><dt><b>Mounted file system</b></dt>
<dd><p>The disk or one of its slices contains a file system that is currently mounted. To correct this error, use the <tt>umount</tt> command.</p></dd>
<dt><b>File system in /etc/vfstab</b></dt>
<dd><p>The disk contains a file system that is listed in the <tt>/etc/vfstab</tt> file, but the file system is not currently mounted. To correct this error, remove or comment out the line in the <tt>/etc/vfstab</tt> file.</p></dd>
<dt><b>Dedicated dump device</b></dt>
<dd><p>The disk is in use as the dedicated dump device for the system. To correct this error, use the <tt>dumpadm</tt> command.</p></dd>
<dt><b>Part of a ZFS pool</b></dt>
<dd><p>The disk or file is part of an active ZFS storage pool. To correct this error, use the <tt>zpool</tt> command to destroy the pool.</p></dd>
</dl>
<p>The following in-use checks serve as helpful warnings and can be overridden by
using the <tt>-f</tt> option to create the pool:</p><dl><dt><b>Contains a file system</b></dt>
<dd><p>The disk contains a known file system, though it is not mounted and doesn't appear to be in use.</p></dd>
<dt><b>Part of volume</b></dt>
<dd><p>The disk is part of an SVM volume.</p></dd>
<dt><b>Live upgrade</b></dt>
<dd><p>The disk is in use as an alternate boot environment for Solaris Live Upgrade.</p></dd>
<dt><b>Part of exported ZFS pool</b></dt>
<dd><p>The disk is part of a storage pool that has been exported or manually removed from a system. In the latter case, the pool is reported as <tt>potentially active</tt>, as the disk might or might not be a network-attached drive in use by another system. Be cautious when overriding a potentially active pool.</p></dd>
</dl>
<p>The following example demonstrates how the <tt>-f</tt> option is used:</p><pre># <tt><b>zpool create tank c1t0d0</b></tt>
invalid vdev specification
use '-f' to override the following errors:
/dev/dsk/c1t0d0s0 contains a ufs filesystem.
# <tt><b>zpool create -f tank c1t0d0</b></tt></pre><p>Ideally, correct the errors rather than use the <tt>-f</tt> option.</p>

<a name="gazgc"></a><h5>Mismatched Replication Levels</h5>
<p>Creating pools with virtual devices of different replication levels is not recommended. The
<tt>zpool</tt> command tries to prevent you from accidentally creating a pool with mismatched
levels of redundancy. If you try to create a pool with such a
configuration, you see errors similar to the following:<a name="indexterm-135"></a><a name="indexterm-136"></a></p><pre># <tt><b>zpool create tank c1t0d0 mirror c2t0d0 c3t0d0</b></tt>
invalid vdev specification
use '-f' to override the following errors:
mismatched replication level: both disk and mirror vdevs are present
# <tt><b>zpool create tank mirror c1t0d0 c2t0d0 mirror c3t0d0 c4t0d0 c5t0d0</b></tt>
invalid vdev specification
use '-f' to override the following errors:
mismatched replication level: 2-way mirror and 3-way mirror vdevs are present</pre><p>You can override these errors with the <tt>-f</tt> option, though this practice is
not recommended. The command also warns you about creating a mirrored or RAID-Z
pool using devices of different sizes. While this configuration is allowed, mismatched levels
of redundancy result in unused space on the larger device, and requires the
<tt>-f</tt> option to override the warning.</p>

<a name="gazhd"></a><h5>Doing a Dry Run of Storage Pool Creation</h5>
<p>Because creating a pool can fail unexpectedly in different ways, and because formatting
disks is such a potentially harmful action, the <tt>zpool create</tt> command has an
additional option, <tt>-n</tt>, which simulates creating the pool without actually writing data to
disk. This option performs the device in-use checking and replication level validation, and reports
any errors in the process. If no errors are found, you see
output similar to the following:<a name="indexterm-137"></a><a name="indexterm-138"></a><a name="indexterm-139"></a></p><pre># <tt><b>zpool create -n tank mirror c1t0d0 c1t1d0</b></tt>
would create 'tank' with the following layout:

        tank
          mirror
            c1t0d0
            c1t1d0</pre><p>Some errors cannot be detected without actually creating the pool. The most common
example is specifying the same device twice in the same configuration. This error
cannot be reliably detected without writing the data itself, so the <tt>create -n</tt> command
can report success and yet fail to create the pool when run for
real.</p>

<a name="gbeef"></a><h5>Default Mount Point for Storage Pools</h5>
<p>When a pool is created, the default mount point for the root
dataset is <i>/pool-name</i>. This directory must either not exist or be empty. If the
directory does not exist, it is automatically created. If the directory is empty,
the root dataset is mounted on top of the existing directory. To create
a pool with a different default mount point, use the <tt>-m</tt> option
of the <tt>zpool create</tt> command:<a name="indexterm-140"></a><a name="indexterm-141"></a></p><pre># <tt><b>zpool create home c1t0d0</b></tt>
default mountpoint '/home' exists and is not empty
use '-m' option to specify a different default
# <tt><b>zpool create -m /export/zfs home c1t0d0</b></tt></pre><pre># <tt><b>zpool create home c1t0d0</b></tt>
default mountpoint '/home' exists and is not empty
use '-m' option to provide a different default
# <tt><b>zpool create -m /export/zfs home c1t0d0</b></tt></pre><p>This command creates a new pool <tt>home</tt> and the <tt>home</tt> dataset with
a mount point of <tt>/export/zfs</tt>.</p><p>For more information about mount points, see <a href="gaynd.html#gaztn">Managing ZFS Mount Points</a>.</p>

<a name="gammr"></a><h4>Destroying ZFS Storage Pools</h4>
<p>Pools are destroyed by using the <tt>zpool destroy</tt> command. This command destroys the pool
even if it contains mounted datasets.<a name="indexterm-142"></a><a name="indexterm-143"></a><a name="indexterm-144"></a></p><pre># <tt><b>zpool destroy tank</b></tt></pre>
<hr><p><b>Caution - </b>Be very careful when you destroy a pool. Make sure you are destroying
the right pool and you always have copies of your data. If
you accidentally destroy the wrong pool, you can attempt to recover the pool. For
more information, see <a href="gbchy.html#gcfhw">Recovering Destroyed ZFS Storage Pools</a>.</p>
<hr>


<a name="gazhm"></a><h5>Destroying a Pool With Faulted Devices</h5>
<p>The act of destroying a pool requires that data be written to
disk to indicate that the pool is no longer valid. This state information
prevents the devices from showing up as a potential pool when you perform
an import. If one or more devices are unavailable, the pool can still
be destroyed. However, the necessary state information won't be written to these damaged
devices.</p><p>These devices, when suitably repaired, are reported as <b>potentially active</b> when you create
a new pool, and appear as valid devices when you search for pools
to import. If a pool has enough faulted devices such that the pool
itself is faulted (meaning that a top-level virtual device is faulted), then the
command prints a warning and cannot complete without the <tt>-f</tt> option. This option
is necessary because the pool cannot be opened, so whether data is stored
there or not is unknown. For example:</p><pre># <tt><b>zpool destroy tank</b></tt>
cannot destroy 'tank': pool is faulted
use '-f' to force destruction anyway
# <tt><b>zpool destroy -f tank</b></tt></pre><p>For more information about pool and device health, see <a href="gaynp.html#gamno">Determining the Health Status of ZFS Storage Pools</a>.</p><p>For more information about importing pools, see <a href="gbchy.html#gazuf">Importing ZFS Storage Pools</a>.</p>
         </div>
      </td>
   </tr>

   <tr class="PageControls" valign="top">
      <td></td>
      <td>
         <table width="100%">
      	   <tr>
      	     <td>
                 <a href="gcfof.html">Previous</a>
             </td>
             <td align="right">
                 <a href="gayrd.html">Next</a>
             </td>
           </tr>
         </table>
      </td>
   </tr>
</tbody>
</table>


</body>
</html>


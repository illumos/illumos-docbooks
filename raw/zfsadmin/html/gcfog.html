<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-type" content="text/html; charset=iso-8859-1">
<title>Components of a ZFS Storage Pool - Solaris ZFS Administration Guide</title>
<meta name="robots" content="index,follow">
<meta name="robots" content="index,follow">
<meta name="date" content="2008-01-01">
<meta name="collection" content="reference">
<link rel="stylesheet" type="text/css" href="css/elements.css">
<link rel="stylesheet" type="text/css" href="css/indiana.css">
</head>

<body>


<div class="Masthead">
   <div class="MastheadLogo"></div>
   <div class="Title">Solaris ZFS Administration Guide</div>
</div>

<table class="Layout" border="0" cellspacing="0" width="100%">
<tbody>

   <tr valign="top" class="PageControls">
      <td></td>
      <td>
         <table width="100%">
      	   <tr>
      	     <td>
                 <a href="gavwn.html">Previous</a>
             </td>
             <td align="right">
                 <a href="gcfof.html">Next</a>
             </td>
           </tr>
         </table>
      </td>
   </tr>
   
   <tr valign="top">
      <td class="Navigation" width="200px"><p class="toc level1"><a href="docinfo.html">Document Information</a></p>
<p class="toc level1 tocsp"><a href="preface-1.html">Preface</a></p>
<p class="toc level1 tocsp"><a href="zfsover-1.html">1.&nbsp;&nbsp;Solaris ZFS File System (Introduction)</a></p>
<p class="toc level1 tocsp"><a href="setup-1.html">2.&nbsp;&nbsp;Getting Started With ZFS</a></p>
<p class="toc level1 tocsp"><a href="gbcik.html">3.&nbsp;&nbsp;ZFS and Traditional File System Differences</a></p>
<p class="toc level1 tocsp"><a href="gavwn.html">4.&nbsp;&nbsp;Managing ZFS Storage Pools</a></p>
<div class="onpage">
<p class="toc level2"><a href="">Components of a ZFS Storage Pool</a></p>
</div>
<p class="toc level2"><a href="gcfof.html">Replication Features of a ZFS Storage Pool</a></p>
<p class="toc level2"><a href="gaypw.html">Creating and Destroying ZFS Storage Pools</a></p>
<p class="toc level2"><a href="gayrd.html">Managing Devices in ZFS Storage Pools</a></p>
<p class="toc level2"><a href="gfifk.html">Managing ZFS Storage Pool Properties</a></p>
<p class="toc level2"><a href="gaynp.html">Querying ZFS Storage Pool Status</a></p>
<p class="toc level2"><a href="gbchy.html">Migrating ZFS Storage Pools</a></p>
<p class="toc level1 tocsp"><a href="gavwq.html">5.&nbsp;&nbsp;Managing ZFS File Systems</a></p>
<p class="toc level1 tocsp"><a href="gavvx.html">6.&nbsp;&nbsp;Working With ZFS Snapshots and Clones</a></p>
<p class="toc level1 tocsp"><a href="ftyxi.html">7.&nbsp;&nbsp;Using ACLs to Protect ZFS Files</a></p>
<p class="toc level1 tocsp"><a href="gbchv.html">8.&nbsp;&nbsp;ZFS Delegated Administration</a></p>
<p class="toc level1 tocsp"><a href="ftyxh.html">9.&nbsp;&nbsp;ZFS Advanced Topics</a></p>
<p class="toc level1 tocsp"><a href="gavwg.html">10.&nbsp;&nbsp;ZFS Troubleshooting and Data Recovery</a></p>
<p class="toc level1 tocsp"><a href="idx-1.html">Index</a></p>
</td>
      <td class="ContentPane" width="705px">

	 <div class="MainContent">      	 
             

<a name="gcfog"></a><h3>Components of a ZFS Storage Pool</h3>
<p>The following sections provide detailed information about the following storage pool components:</p>
<ul><li><p><a href="#gazdp">Using Disks in a ZFS Storage Pool</a></p></li>
<li><p><a href="#gazcr">Using Files in a ZFS Storage Pool</a></p></li>
<li><p><a href="#gazca">Identifying Virtual Devices in a Storage Pool</a></p></li></ul>


<a name="gazdp"></a><h4>Using Disks in a ZFS Storage Pool</h4>
<p>The most basic element of a storage pool is a piece of
physical storage. Physical storage can be any block device of at least 128
Mbytes in size. Typically, this device is a hard drive that is visible
to the system in the <tt>/dev/dsk</tt> directory.<a name="indexterm-89"></a><a name="indexterm-90"></a></p><p>A storage device can be a whole disk (<tt>c1t0d0</tt>) or an individual slice
(<tt>c0t0d0s7</tt>). The recommended mode of operation is to use an entire disk, in
which case the disk does not need to be specially formatted. ZFS formats
the disk using an EFI label to contain a single, large slice. When
used in this way, the partition table that is displayed by the <tt>format</tt>
command appears similar to the following:</p><pre>Current partition table (original):
Total disk sectors available: 71670953 + 16384 (reserved sectors)

Part      Tag    Flag     First Sector        Size        Last Sector
  0        usr    wm                34      34.18GB         71670953    
  1 unassigned    wm                 0          0              0    
  2 unassigned    wm                 0          0              0    
  3 unassigned    wm                 0          0              0    
  4 unassigned    wm                 0          0              0    
  5 unassigned    wm                 0          0              0    
  6 unassigned    wm                 0          0              0    
  7 unassigned    wm                 0          0              0    
  8   reserved    wm          71670954       8.00MB         71687337</pre><p>To use whole disks, the disks must be named using the standard
Solaris convention, such as <tt>/dev/dsk/cXtXdXsX</tt>. Some third-party drivers use a different naming convention or
place disks in a location other than the <tt>/dev/dsk</tt> directory. To use these
disks, you must manually label the disk and provide a slice to ZFS.
<a name="indexterm-91"></a><a name="indexterm-92"></a></p><p>ZFS applies an EFI label when you create a storage pool with
whole disks. Disks can be labeled with a traditional Solaris VTOC label when
you create a storage pool with a disk slice.</p><p>Slices should only be used under the following conditions:</p>
<ul><li><p>The device name is nonstandard.</p></li>
<li><p>A single disk is shared between ZFS and another file system, such as UFS.</p></li>
<li><p>A disk is used as a swap or a dump device.</p></li></ul>
<p>Disks can be specified by using either the full path, such as
<tt>/dev/dsk/c1t0d0</tt>, or a shorthand name that consists of the device name within
the <tt>/dev/dsk</tt> directory, such as <tt>c1t0d0</tt>. For example, the following are valid disk
names:<a name="indexterm-93"></a><a name="indexterm-94"></a><a name="indexterm-95"></a></p>
<ul><li><p><tt>c1t0d0</tt></p></li>
<li><p><tt>/dev/dsk/c1t0d0</tt></p></li>
<li><p><tt>c0t0d6s2</tt></p></li>
<li><p><tt>/dev/foo/disk</tt></p></li></ul>
<p>Using whole physical disks is the simplest way to create ZFS storage pools.
ZFS configurations become progressively more complex, from management, reliability, and performance perspectives, when
you build pools from disk slices, LUNs in hardware RAID arrays, or volumes
presented by software-based volume managers. The following considerations might help you determine how to
configure ZFS with other hardware or software storage solutions:</p>
<ul><li><p>If you construct ZFS configurations on top of LUNs from hardware RAID arrays, you need to understand the relationship between ZFS redundancy features and the redundancy features offered by the array. Certain configurations might provide adequate redundancy and performance, but other configurations might not.</p></li>
<li><p>You can construct logical devices for ZFS using volumes presented by software-based volume managers, such as Solaris<sup>TM</sup> Volume Manager (SVM) or Veritas Volume Manager (VxVM). However, these configurations are not recommended. While ZFS functions properly on such devices, less-than-optimal performance might be the result.</p></li></ul>
<p>For additional information about storage pool recommendations, see the ZFS best practices site:</p><p><a href="http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide">http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide</a></p><p>Disks are identified both by their path and by their device ID,
if available. This method allows devices to be reconfigured on a system without
having to update any ZFS state. If a disk is switched between controller
1 and controller 2, ZFS uses the device ID to detect that the
disk has moved and should now be accessed using controller 2. The device
ID is unique to the drive's firmware. While unlikely, some firmware updates have
been known to change device IDs. If this situation happens, ZFS can still
access the device by path and update the stored device ID automatically. If
you inadvertently change both the path and the ID of the device, then
export and re-import the pool in order to use it.</p>

<a name="gazcr"></a><h4>Using Files in a ZFS Storage Pool</h4>
<p>ZFS also allows you to use UFS files as virtual devices in
your storage pool. This feature is aimed primarily at testing and enabling simple
experimentation, not for production use. The reason is that <b>any use of files relies on the underlying file system for consistency</b>. If you create a
ZFS pool backed by files on a UFS file system, then you
are implicitly relying on UFS to guarantee correctness and synchronous semantics.<a name="indexterm-96"></a><a name="indexterm-97"></a></p><p>However, files can be quite useful when you are first trying out
ZFS or experimenting with more complicated layouts when not enough physical devices are present.
All files must be specified as complete paths and must be at least
64 Mbytes in size. If a file is moved or renamed, the pool
must be exported and re-imported in order to use it, as no device
ID is associated with files by which they can be located.</p>

<a name="gazca"></a><h4>Identifying Virtual Devices in a Storage Pool</h4>
<p>Each storage pool is comprised of one or more virtual devices. A
<b>virtual device</b> is an internal representation of the storage pool that describes the layout
of physical storage and its fault characteristics. As such, a virtual device represents
the disk devices or files that are used to create the storage pool.
<a name="indexterm-98"></a><a name="indexterm-99"></a></p><p>Two top-level virtual devices provide data redundancy: mirror and RAID-Z virtual devices. These
virtual devices consist of disks, disk slices, or files.</p><p>Disks, disk slices, or files that are used in pools outside of
mirrors and RAID-Z virtual devices, function as top-level virtual devices themselves.</p><p>Storage pools typically contain multiple top-level virtual devices. ZFS dynamically stripes data among
all of the top-level virtual devices in a pool.</p>
         </div>
      </td>
   </tr>

   <tr class="PageControls" valign="top">
      <td></td>
      <td>
         <table width="100%">
      	   <tr>
      	     <td>
                 <a href="gavwn.html">Previous</a>
             </td>
             <td align="right">
                 <a href="gcfof.html">Next</a>
             </td>
           </tr>
         </table>
      </td>
   </tr>
</tbody>
</table>


</body>
</html>


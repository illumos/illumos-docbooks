<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-type" content="text/html; charset=iso-8859-1">
<title>What's New in ZFS? - Solaris ZFS Administration Guide</title>
<meta name="robots" content="index,follow">
<meta name="robots" content="index,follow">
<meta name="date" content="2008-01-01">
<meta name="collection" content="reference">
<link rel="stylesheet" type="text/css" href="css/elements.css">
<link rel="stylesheet" type="text/css" href="css/indiana.css">
</head>

<body>


<div class="Masthead">
   <div class="MastheadLogo"></div>
   <div class="Title">Solaris ZFS Administration Guide</div>
</div>

<table class="Layout" border="0" cellspacing="0" width="100%">
<tbody>

   <tr valign="top" class="PageControls">
      <td></td>
      <td>
         <table width="100%">
      	   <tr>
      	     <td>
                 <a href="zfsover-1.html">Previous</a>
             </td>
             <td align="right">
                 <a href="zfsover-2.html">Next</a>
             </td>
           </tr>
         </table>
      </td>
   </tr>
   
   <tr valign="top">
      <td class="Navigation" width="200px"><p class="toc level1"><a href="docinfo.html">Document Information</a></p>
<p class="toc level1 tocsp"><a href="preface-1.html">Preface</a></p>
<p class="toc level1 tocsp"><a href="zfsover-1.html">1.&nbsp;&nbsp;Solaris ZFS File System (Introduction)</a></p>
<div class="onpage">
<p class="toc level2"><a href="">What's New in ZFS?</a></p>
</div>
<p class="toc level2"><a href="zfsover-2.html">What Is ZFS?</a></p>
<p class="toc level2"><a href="ftyue.html">ZFS Terminology</a></p>
<p class="toc level2"><a href="gbcpt.html">ZFS Component Naming Requirements</a></p>
<p class="toc level1 tocsp"><a href="setup-1.html">2.&nbsp;&nbsp;Getting Started With ZFS</a></p>
<p class="toc level1 tocsp"><a href="gbcik.html">3.&nbsp;&nbsp;ZFS and Traditional File System Differences</a></p>
<p class="toc level1 tocsp"><a href="gavwn.html">4.&nbsp;&nbsp;Managing ZFS Storage Pools</a></p>
<p class="toc level1 tocsp"><a href="gavwq.html">5.&nbsp;&nbsp;Managing ZFS File Systems</a></p>
<p class="toc level1 tocsp"><a href="gavvx.html">6.&nbsp;&nbsp;Working With ZFS Snapshots and Clones</a></p>
<p class="toc level1 tocsp"><a href="ftyxi.html">7.&nbsp;&nbsp;Using ACLs to Protect ZFS Files</a></p>
<p class="toc level1 tocsp"><a href="gbchv.html">8.&nbsp;&nbsp;ZFS Delegated Administration</a></p>
<p class="toc level1 tocsp"><a href="ftyxh.html">9.&nbsp;&nbsp;ZFS Advanced Topics</a></p>
<p class="toc level1 tocsp"><a href="gavwg.html">10.&nbsp;&nbsp;ZFS Troubleshooting and Data Recovery</a></p>
<p class="toc level1 tocsp"><a href="idx-1.html">Index</a></p>
</td>
      <td class="ContentPane" width="705px">

	 <div class="MainContent">      	 
             

<a name="gbscy"></a><h3>What's New in ZFS?</h3>
<p>This section summarizes new features in the ZFS file system.</p>
<ul><li><p><a href="#gfxul">Using Cache Devices in Your ZFS Storage Pool</a></p></li>
<li><p><a href="#gfwqj">Enhancements to the <tt>zfs send</tt> Command</a></p></li>
<li><p><a href="#gfwpz">ZFS Quotas and Reservations for File System Data Only</a></p></li>
<li><p><a href="#gftgg">ZFS File System Properties for the Solaris CIFS Service</a></p></li>
<li><p><a href="#gftgp">ZFS Storage Pool Properties</a></p></li>
<li><p><a href="#gfwqw">ZFS and File System Mirror Mounts</a></p></li>
<li><p><a href="#gfiit">ZFS Command History Enhancements (<tt>zpool history</tt>)</a></p></li>
<li><p><a href="#gffys">Upgrading ZFS File Systems (<tt>zfs upgrade</tt>)</a></p></li>
<li><p><a href="#gfgak">ZFS Delegated Administration</a></p></li>
<li><p><a href="#gfgaa">Setting Up Separate ZFS Logging Devices</a></p></li>
<li><p><a href="#gfgam">Creating Intermediate ZFS Datasets</a></p></li>
<li><p><a href="#gfgac">ZFS Hotplugging Enhancements</a></p></li>
<li><p><a href="#gevnp">Recursively Renaming ZFS Snapshots (<tt>zfs rename</tt> <tt>-r</tt>)</a></p></li>
<li><p><a href="#gevpi">GZIP Compression is Available for ZFS</a></p></li>
<li><p><a href="#gevpg">Storing Multiple Copies of ZFS User Data</a></p></li>
<li><p><a href="#gebws">Improved <tt>zpool status</tt> Output</a></p></li>
<li><p><a href="#gebwq">ZFS and Solaris iSCSI Improvements</a></p></li>
<li><p><a href="#gebwp">Sharing ZFS File System Enhancements</a></p></li>
<li><p><a href="#gdswe">ZFS Command History (<tt>zpool history</tt>)</a></p></li>
<li><p><a href="#gdswf">ZFS Property Improvements</a></p></li>
<li><p><a href="#gdsvh">Displaying All ZFS File System Information</a></p></li>
<li><p><a href="#gdsup">New <tt>zfs receive</tt> <tt>-F</tt> Option</a></p></li>
<li><p><a href="#gdfdt">Recursive ZFS Snapshots</a></p></li>
<li><p><a href="#gcviu">Double Parity RAID-Z (<tt>raidz2</tt>)</a></p></li>
<li><p><a href="#gcvdm">Hot Spares for ZFS Storage Pool Devices</a></p></li>
<li><p><a href="#gcvgc">Replacing a ZFS File System With a ZFS Clone (<tt>zfs promote</tt>)</a></p></li>
<li><p><a href="#gcvit">Upgrading ZFS Storage Pools (<tt>zpool upgrade</tt>)</a></p></li>
<li><p><a href="#gcsxk">Using ZFS to Clone Non-Global Zones and Other Enhancements</a></p></li>
<li><p><a href="#gciui">ZFS Backup and Restore Commands are Renamed</a></p></li>
<li><p><a href="#gcitn">Recovering Destroyed Storage Pools</a></p></li>
<li><p><a href="#gcfhy">ZFS is Integrated With Fault Manager</a></p></li>
<li><p><a href="#gcfiw">New <tt>zpool clear</tt> Command</a></p></li>
<li><p><a href="#gcajn">Compact NFSv4 ACL Format</a></p></li>
<li><p><a href="#gcakl">File System Monitoring Tool (<tt>fsstat</tt>)</a></p></li>
<li><p><a href="#gbsbp">ZFS Web-Based Management</a></p></li></ul>


<a name="gfxul"></a><h4>Using Cache Devices in Your ZFS Storage Pool</h4>
<p><b>Solaris Express Developer Edition 1/08:</b> In this Solaris release, you can create pool and specify <b>cache devices</b>, which are
used to cache storage pool data.</p><p>Cache devices provide an additional layer of caching between main memory and disk.
Using cache devices provide the greatest performance improvement for random read-workloads of mostly
static content.</p><p>One or more cache devices can specified when the pool is created.
For example:</p><pre># zpool create pool mirror c0t2d0 c0t4d0 cache c0t0d0
# zpool status pool
  pool: pool
 state: ONLINE
 scrub: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        pool        ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c0t2d0  ONLINE       0     0     0
            c0t4d0  ONLINE       0     0     0
        cache
          c0t0d0    ONLINE       0     0     0

errors: No known data errors</pre><p>After cache devices are added, they gradually fill with content from main memory.
Depending on the size of your cache device, it could take over an
hour for them to fill. Capacity and reads can be monitored by
using the <tt>zpool iostat</tt> command as follows:</p><pre># zpool iostat -v pool 5</pre><p>Cache devices can be added or removed from the pool after the
pool is created.</p><p>For more information, see <a href="gaypw.html#gfxtd">Creating a ZFS Storage Pool with Cache Devices</a> and <a href="gayrd.html#gfxrx">Example&nbsp;4-3</a>.</p>

<a name="gfwqj"></a><h4>Enhancements to the <tt>zfs send</tt> Command</h4>
<p><b>Solaris Express Developer Edition 1/08:</b> This release includes the following enhancements to the <tt>zfs send</tt> command.</p>
<ul><li><p>Send all incremental streams from one snapshot to a cumulative snapshot. For example:</p><pre># zfs list
NAME                      USED  AVAIL  REFER  MOUNTPOINT
pool                      428K  16.5G    20K  /pool
pool/fs                    71K  16.5G    21K  /pool/fs
pool/fs@snapA              16K      -  18.5K  -
pool/fs@snapB              17K      -    20K  -
pool/fs@snapC              17K      -  20.5K  -
pool/fs@snapD                0      -    21K  -
# zfs send -I pool/fs@snapA pool/fs@snapD > /snaps/fs@combo</pre><p>Send all incremental snapshots between <tt>fs@snapA</tt> to <tt>fs@snapD</tt> to <tt>fs@combo</tt>.</p></li>
<li><p>Send an incremental stream from the origin snapshot to create a clone. The original snapshot must already exist on the receiving side to accept the incremental stream. For example:</p><pre># zfs send -I pool/fs@snap1 pool/clone@snapA > /snaps/fsclonesnap-I
.
.
# zfs receive -F pool/clone &lt; /snaps/fsclonesnap-I</pre></li>
<li><p>Send a replication stream of all descendent file systems, up to the named snapshots. When received, all properties, snapshots, descendent file systems, and clones are preserved. For example:</p><pre>zfs send -R pool/fs@snap > snaps/fs-R</pre><p>For an extended example, see <a href="gbchx.html#gfxpm">Example&nbsp;6-1</a>.</p></li>
<li><p>Send an incremental replication stream. </p><pre>zfs send -R -[iI] @snapA pool/fs@snapD</pre><p>For an extended example, see <a href="gbchx.html#gfxpm">Example&nbsp;6-1</a>.</p></li></ul>
<p>For more information, see <a href="gbchx.html#gfwqb">Sending and Receiving Complex ZFS Snapshot Streams</a>.</p>

<a name="gfwpz"></a><h4>ZFS Quotas and Reservations for File System Data Only</h4>
<p><b>Solaris Express Developer Edition 1/08:</b> In addition to the existing ZFS quota and reservation features, this release includes
dataset quotas and reservations that do not include descendents, such as snapshots and
clones, in the space consumption accounting.</p>
<ul><li><p>The <tt>refquota</tt> property limits the amount of space a dataset can consume. This property enforces a hard limit on the amount of space that can be used. This hard limit does not include space used by descendents, such as snapshots and clones.</p></li>
<li><p>The <tt>refreservation</tt> property sets the minimum amount of space that is guaranteed to a dataset, not including its descendents.</p></li></ul>
<p>For example, you can set a 10 Gbyte <tt>refquota</tt> for <tt>studentA</tt> that
sets a 10-Gbyte hard limit of <b>referenced</b> space. For additional flexibility, you can
set a 20-Gbyte quota that allows you to manage <tt>studentA</tt>'s snapshots.</p><pre># zfs set refquota=10g tank/studentA
# zfs set quota=20g tank/studentA</pre><p>For more information, see <a href="gazvb.html">ZFS Quotas and Reservations</a>.</p>

<a name="gftgg"></a><h4>ZFS File System Properties for the Solaris CIFS Service</h4>
<p><b>Solaris Express Developer Edition 1/08:</b> This release provides support for the Solaris Common Internet File System (CIFS) service.
This product provides the ability to share files between Solaris and Windows or
MacOS systems.</p><p>To facilitate sharing files between these systems by using the Solaris CIFS service,
the following new ZFS properties are provided:</p>
<ul><li><p>Case sensitivity support (<tt>casesensitivity</tt>)</p></li>
<li><p>Non-blocking mandatory locks (<tt>nbmand</tt>)</p></li>
<li><p>SMB share support (<tt>sharesmb</tt>)</p></li>
<li><p>Unicode normalization support (<tt>normalization</tt>)</p></li>
<li><p>UTF-8 character set support (<tt>utf8only</tt>)</p></li></ul>
<p>Currently, the <tt>sharesmb</tt> property is available to share ZFS files in the Solaris
CIFS environment. More ZFS CIFS-related properties will be available in an upcoming release.
For information about using the <tt>sharesmb</tt> property, see <a href="gaynd.html#gfwqv">Sharing ZFS Files in a Solaris CIFS Environment</a>.</p><p>In addition to the ZFS properties added for supporting the Solaris CIFS software
product, the <tt>vscan</tt> property is available for scanning ZFS files if you have
a 3rd-party virus scanning engine.</p>

<a name="gftgp"></a><h4>ZFS Storage Pool Properties</h4>
<p><b>Solaris Express Developer Edition 1/08:</b> ZFS storage pool properties were introduced in an earlier release. This release provides
for additional property information. For example:</p><pre># zpool get all users
NAME   PROPERTY     VALUE       SOURCE
users  size         16.8G       -
users  used         217M        -
users  available    16.5G       -
users  capacity     1%          -
users  altroot      -           default
users  health       ONLINE      -
users  guid         11063207170669925585  -
users  version      8           default
users  bootfs       -           default
users  delegation   on          default
users  autoreplace  off         default
users  temporary    on          local</pre><p>For a description of these properties, see <a href="gfifk.html#gfiex">Table&nbsp;4-1</a>.</p>
<ul><li><p>The <tt>cachefile</tt> property &ndash; <b>Solaris Express Developer Edition 1/08:</b> This release provides the <tt>cachefile</tt> property, which controls where pool configuration information is cached. All pools in the cache are automatically imported when the system boots. However, installation and clustering environments might need to cache this information in a different location so that pools are not automatically imported.</p><p>You can set this property to cache pool configuration in a different location that can be imported later by using the <tt>zpool import</tt> <tt>c</tt> command. For most ZFS configurations, this property would not be used.</p><p>The <tt>cachefile</tt> property is not persistent and is not stored on disk. This property replaces the <tt>temporary</tt> property that was used to indicate that pool information should not be cached in previous Solaris releases.</p></li>
<li><p>The <tt>failmode</tt> property &ndash; <b>Solaris Express Developer Edition 1/08:</b> This release provides the <tt>failmode</tt> property for determining the behavior of a catastrophic pool failure due to a loss of device connectivity or the failure of all devices in the pool. The <tt>failmode</tt> property can be set to these values: <tt>wait</tt>, <tt>continue</tt>, or <tt>panic</tt>. The default value is <tt>wait</tt>, which means you must reconnect the device or replace a failed device and clear the error with the <tt>zpool clear</tt> command.</p><p>The <tt>failmode</tt> property is set like other settable ZFS properties, which can be set either before or after the pool is created. For example:</p><pre># zpool set failmode=continue tank
# zpool get failmode tank
NAME  PROPERTY  VALUE     SOURCE
tank  failmode  continue  local</pre><pre># zpool create -o failmode=continue</pre><p>For a description of all ZFS pool properties, see <a href="gfifk.html#gfiex">Table&nbsp;4-1</a>.</p></li></ul>


<a name="gfwqw"></a><h4>ZFS and File System Mirror Mounts</h4>
<p><b>Solaris Express Developer Edition 1/08:</b> In this Solaris release, NFSv4 mount enhancements are provided to make ZFS
file systems more accessible to NFS clients.<a name="indexterm-1"></a><a name="indexterm-2"></a><a name="indexterm-3"></a></p><p>When file systems are created on the NFS server, the NFS client
can automatically discover these newly created file systems within their existing mount of a
parent file system.</p><p>For example, if the server <tt>neo</tt> already shares the <tt>tank</tt> file system
and client <tt>zee</tt> has it mounted, <tt>/tank/baz</tt> is automatically visible on the
client after it is created on the server. </p><pre>zee# mount neo:/tank /mnt
zee# ls /mnt
baa    bar

neo# zfs create tank/baz

zee% ls /mnt
baa    bar    baz
zee% ls /mnt/baz
file1    file2 </pre>

<a name="gfiit"></a><h4>ZFS Command History Enhancements (<tt>zpool history</tt>)</h4>
<p><b>Solaris Express Developer Edition 9/07:</b> The <tt>zpool history</tt> command has been enhanced to provide the following new features:</p>
<ul><li><p>ZFS file system event information</p></li>
<li><p>A <tt>-l</tt> option for displaying a long format that includes the user name, the hostname, and the zone in which the operation was performed </p></li>
<li><p>A <tt>-i</tt> option for displaying internal event information that can be used for diagnostic purposes</p></li></ul>
<p>For example, the <tt>zpool history</tt> command provides both <tt>zpool</tt> command events and <tt>zfs</tt>
command events.</p><pre># zpool history users
History for 'users':
2007-04-26.12:44:02 zpool create users mirror c0t8d0 c0t9d0 c0t10d0
2007-04-26.12:44:38 zfs create users/markm
2007-04-26.12:44:47 zfs create users/marks
2007-04-26.12:44:57 zfs create users/neil
2007-04-26.12:47:15 zfs snapshot -r users/home@yesterday
2007-04-26.12:54:50 zfs snapshot -r users/home@today
2007-04-26.13:29:13 zfs create users/snapshots
2007-04-26.13:30:00 zfs create -o compression=gzip users/snapshots
2007-04-26.13:31:24 zfs create -o compression=gzip-9 users/oldfiles
2007-04-26.13:31:47 zfs set copies=2 users/home
2007-06-25.14:22:52 zpool offline users c0t10d0
2007-06-25.14:52:42 zpool online users c0t10d0
2007-06-25.14:53:06 zpool upgrade users</pre><p>The <tt>zpool history</tt> <tt>-i</tt> option provides internal event information. For example:</p><pre># zpool history -i

.
.
.
2007-08-08.15:10:02 [internal create txg:348657] dataset = 83
2007-08-08.15:10:03 zfs create tank/mark
2007-08-08.15:27:41 [internal permission update txg:348869] ul$76928 create dataset = 5
2007-08-08.15:27:41 [internal permission update txg:348869] ul$76928 destroy dataset = 5
2007-08-08.15:27:41 [internal permission update txg:348869] ul$76928 mount dataset = 5
2007-08-08.15:27:41 [internal permission update txg:348869] ud$76928 create dataset = 5
2007-08-08.15:27:41 [internal permission update txg:348869] ud$76928 destroy dataset = 5
2007-08-08.15:27:41 [internal permission update txg:348869] ud$76928 mount dataset = 5
2007-08-08.15:27:41 zfs allow marks create,destroy,mount tank
2007-08-08.15:27:59 [internal permission update txg:348873] ud$76928 snapshot dataset = 5
2007-08-08.15:27:59 zfs allow -d marks snapshot tank</pre><p>The <tt>zpool history</tt> <tt>-l</tt> option provides a long format. For example:</p><pre># zpool history -l tank
History for 'tank':
2007-07-19.10:55:13 zpool create tank mirror c0t1d0 c0t11d0 [user root on neo:global]
2007-07-19.10:55:19 zfs create tank/cindys [user root on neo:global]
2007-07-19.10:55:49 zfs allow cindys create,destroy,mount,snapshot tank/cindys [user root on neo:global]
2007-07-19.10:56:24 zfs create tank/cindys/data [user cindys on neo:global]</pre><p>For more information about using the <tt>zpool history</tt> command, see <a href="gbbuw.html">Identifying Problems in ZFS</a>.</p>

<a name="gffys"></a><h4>Upgrading ZFS File Systems (<tt>zfs upgrade</tt>)</h4>
<p><b>Solaris Express Developer Edition 9/07:</b> The <tt>zfs upgrade</tt> command is included in this release to provide future ZFS
file system enhancements to existing file systems. ZFS storage pools have a similar
upgrade feature to provide pool enhancements to existing storage pools.</p><p>For example:</p><pre># zfs upgrade
This system is currently running ZFS filesystem version 2.

The following filesystems are out of date, and can be upgraded.  After being
upgraded, these filesystems (and any 'zfs send' streams generated from
subsequent snapshots) will no longer be accessible by older software versions.

VER  FILESYSTEM
---  ------------
 1   datab
 1   datab/users
 1   datab/users/area51</pre>
<hr><p><b>Note - </b>File systems that are upgraded and any streams created from those upgraded file
systems by the <tt>zfs send</tt> command are not accessible on systems that are running
older software releases.</p>
<hr>
<p>However, no new ZFS file system upgrade features are provided in this release.</p>

<a name="gfgak"></a><h4>ZFS Delegated Administration</h4>
<p><b>Solaris Express Developer Edition 9/07:</b> In this release, you can delegate fine-grained permissions to perform ZFS administration tasks
to non-privileged users.</p><p>You can use the <tt>zfs allow</tt> and <tt>zfs unallow</tt> commands to grant and remove permissions.</p><p>You can modify the ability to use delegated administration with the pool's <tt>delegation</tt>
property. For example:</p><pre># zpool get delegation users
NAME  PROPERTY    VALUE       SOURCE
users  delegation  on          default
# zpool set delegation=off users
# zpool get delegation users
NAME  PROPERTY    VALUE       SOURCE
users  delegation  off         local</pre><p>By default, the <tt>delegation</tt> property is enabled.</p><p>For more information, see <a href="gbchv.html">Chapter&nbsp;8, ZFS Delegated Administration</a> and <a href="http://docs.sun.com/doc/819-2240/zfs-1m?a=view"><tt>zfs</tt>(1M)</a>.</p>

<a name="gfgaa"></a><h4>Setting Up Separate ZFS Logging Devices</h4>
<p><b>Solaris Express Developer Edition 9/07:</b> The ZFS intent log (ZIL) is provided to satisfy POSIX requirements for
synchronous transactions. For example, databases often require their transactions to be on stable storage
devices when returning  from a system call. NFS and other applications can
also use fsync() to ensure data stability. By default,  the ZIL is
allocated  from blocks within the main storage pool. However, better performance might
be possible by using separate intent log devices in your ZFS storage pool,
such as with NVRAM or a dedicated disk.<a name="indexterm-4"></a><a name="indexterm-5"></a></p><p>Log devices for the ZFS intent log are not related to database
log files.</p><p>You can set up a ZFS logging device when the storage pool
is created or after the pool is created. For examples of setting up
log devices, see <a href="gaypw.html#gffyt">Creating a ZFS Storage Pool with Log Devices</a> and <a href="gayrd.html#gazgw">Adding Devices to a Storage Pool</a>.</p><p>You can attach a log device to an existing log device to
create a mirrored log device. This operation is identical to attaching a device
in a unmirrored storage pool.</p><p>Consider the following points when determining whether setting up a ZFS log device
is appropriate for your environment:</p>
<ul><li><p>Any performance improvement seen by implementing a separate log device depends on the device type, the hardware configuration of the pool, and the application workload. For preliminary performance information, see this blog:</p><p><a href="http://blogs.sun.com/perrin/entry/slog_blog_or_blogging_on">http://blogs.sun.com/perrin/entry/slog_blog_or_blogging_on</a></p></li>
<li><p>Log devices can be unreplicated or mirrored, but RAIDZ is not supported for log devices.</p></li>
<li><p>If a separate log device is not mirrored and the device that contains the log fails, storing log blocks reverts to the storage pool.</p></li>
<li><p>Log devices can be added, replaced, attached, detached, and imported and exported as part of the larger storage pool. Currently, log devices cannot be removed.</p></li>
<li><p>The minimum size of a log device is the same as the minimum size of device in pool, which is 64 Mbytes. The amount of in-play data that might be stored on a log device is relatively small. Log blocks are freed when the log transaction (system call) is committed.</p></li>
<li><p>The maximum size of a log device should be approximately 1/2 the size of physical memory because that is the maximum amount of potential in-play data that can be stored. For example, if a system has 16 Gbytes of physical memory, consider a maximum log device size of 8 Gbytes.</p></li></ul>


<a name="gfgam"></a><h4>Creating Intermediate ZFS Datasets</h4>
<p><b>Solaris Express Developer Edition 9/07:</b> You can use the <tt>-p</tt> option with the <tt>zfs create</tt>, <tt>zfs clone</tt>, and <tt>zfs rename</tt>
commands to quickly create a non-existent intermediate dataset, if it doesn't already exist.</p><p>For example, create ZFS datasets (<tt>users/area51</tt>) in the <tt>datab</tt> storage pool.</p><pre># zfs list
NAME                        USED  AVAIL  REFER  MOUNTPOINT
datab                       106K  16.5G    18K  /datab
# zfs create -p -o compression=on datab/users/area51</pre><p>If the intermediate dataset exists during the create operation, the operation completes successfully.</p><p>Properties specified apply to the target dataset, not to the intermediate datasets. For
example:</p><pre># zfs get mountpoint,compression datab/users/area51
NAME                PROPERTY     VALUE                SOURCE
datab/users/area51  mountpoint   /datab/users/area51  default
datab/users/area51  compression  on                   local</pre><p>The intermediate dataset is created with the default mount point. Any additional properties
are disabled for the intermediate dataset. For example:</p><pre># zfs get mountpoint,compression datab/users
NAME         PROPERTY     VALUE         SOURCE
datab/users  mountpoint   /datab/users  default
datab/users  compression  off           default</pre><p>For more information, see <a href="http://docs.sun.com/doc/819-2240/zfs-1m?a=view"><tt>zfs</tt>(1M)</a>.</p>

<a name="gfgac"></a><h4>ZFS Hotplugging Enhancements</h4>
<p><b>Solaris Express Developer Edition 9/07:</b>  In this release, ZFS more effectively responds to devices that are
removed and provides a mechanism to automatically identify devices that are inserted with the
following enhancements:</p>
<ul><li><p>You can replace an existing device with an equivalent device without having to use the <tt>zpool replace</tt> command.</p><p>The <tt>autoreplace</tt> property controls automatic device replacement. If set to off, device replacement must be initiated by the administrator by using the <tt>zpool replace</tt> command. If set to on, any new device, found in the same physical location as a device that previously belonged to the pool, is automatically formatted and replaced. The default behavior is off.</p></li>
<li><p>The storage pool state <tt>REMOVED</tt> is provided when a device or hot spare has been removed if the device was physically removed while the system was running. A hot-spare device is substituted for the removed device, if available.</p></li>
<li><p>If a device is removed and then inserted, the device is placed online. If a hot-spare was activated when the device is re-inserted, the spare is removed when the online operation completes.</p></li>
<li><p>Automatic detection when devices are removed or inserted is hardware-dependent and might not be supported on all platforms. For example, USB devices are automatically configured upon inserted. However, you might have to use the <tt>cfgadm</tt> <tt>-c configure</tt> command to configure a SATA drive.</p></li>
<li><p>Hot spares are checked periodically to make sure they are online and available.</p></li></ul>
<p>For more information, see <a href="http://docs.sun.com/doc/819-2240/zpool-1m?a=view"><tt>zpool</tt>(1M)</a>.</p>

<a name="gevnp"></a><h4>Recursively Renaming ZFS Snapshots (<tt>zfs rename</tt> <tt>-r</tt>)</h4>
<p><b>Solaris Express Developer Edition 5/07:</b> You can recursively rename all descendent ZFS snapshots by using the <tt>zfs rename</tt>
<tt>-r</tt> command. </p><p>For example, snapshot a set of ZFS file systems.</p><pre># <tt><b>zfs snapshot -r users/home@today</b></tt>
# <tt><b>zfs list</b></tt>
NAME                     USED  AVAIL  REFER  MOUNTPOINT
users                    216K  16.5G    20K  /users
users/home                76K  16.5G    22K  /users/home
users/home@today            0      -    22K  -
users/home/markm          18K  16.5G    18K  /users/home/markm
users/home/markm@today      0      -    18K  -
users/home/marks          18K  16.5G    18K  /users/home/marks
users/home/marks@today      0      -    18K  -
users/home/neil           18K  16.5G    18K  /users/home/neil
users/home/neil@today       0      -    18K  -</pre><p>Then, rename the snapshots the following day.</p><pre># <tt><b>zfs rename -r users/home@today @yesterday</b></tt>
# <tt><b>zfs list</b></tt>
NAME                         USED  AVAIL  REFER  MOUNTPOINT
users                        216K  16.5G    20K  /users
users/home                    76K  16.5G    22K  /users/home
users/home@yesterday            0      -    22K  -
users/home/markm              18K  16.5G    18K  /users/home/markm
users/home/markm@yesterday      0      -    18K  -
users/home/marks              18K  16.5G    18K  /users/home/marks
users/home/marks@yesterday      0      -    18K  -
users/home/neil               18K  16.5G    18K  /users/home/neil
users/home/neil@yesterday       0      -    18K  -</pre><p>Snapshots are the only dataset that can be renamed recursively. </p><p>For more information about snapshots, see <a href="gbciq.html">Overview of ZFS Snapshots</a> and this blog entry that
describes how to create rolling snapshots:</p><p><a href="http://blogs.sun.com/mmusante/entry/rolling_snapshots_made_easy">http://blogs.sun.com/mmusante/entry/rolling_snapshots_made_easy</a></p>

<a name="gevpi"></a><h4>GZIP Compression is Available for ZFS</h4>
<p><b>Solaris Express Developer Edition 5/07:</b> In this Solaris release, you can set <tt>gzip</tt> compression on ZFS file systems
in addition to <tt>lzjb</tt> compression. You can specify compression as <tt>gzip</tt>, the default,
or <tt>gzip-</tt><i>N</i>, where <i>N</i> equals 1 through 9. For example:</p><pre># zfs create -o compression=gzip users/home/snapshots
# zfs get compression users/home/snapshots
NAME                  PROPERTY     VALUE            SOURCE
users/home/snapshots  compression  gzip             local
# zfs create -o compression=gzip-9 users/home/oldfiles
# zfs get compression users/home/oldfiles
NAME                  PROPERTY     VALUE           SOURCE
users/home/oldfiles   compression  gzip-9          local</pre><p>For more information about setting ZFS properties, see <a href="gayns.html#gazsp">Setting ZFS Properties</a>.</p>

<a name="gevpg"></a><h4>Storing Multiple Copies of ZFS User Data</h4>
<p><b>Solaris Express Developer Edition 5/07:</b> As a reliability feature, ZFS file system metadata is automatically stored multiple times
across different disks, if possible. This feature is known as <b>ditto blocks</b>.</p><p>In this Solaris release, you can specify that multiple copies of user data
is also stored per file system by using the <tt>zfs set copies</tt> command. For example:</p><pre># <tt><b>zfs set copies=2 users/home</b></tt>
# <tt><b>zfs get copies users/home</b></tt>
NAME        PROPERTY  VALUE       SOURCE
users/home  copies    2           local</pre><p>Available values are 1, 2, or 3. The default value is 1.
These copies are in addition to any pool-level redundancy, such as in a
mirrored or RAID-Z configuration.</p><p>The benefits of storing multiple copies of ZFS user data are as
follows:</p>
<ul><li><p>Improves data retention by allowing recovery from unrecoverable block read faults, such as media faults (bit rot) for all ZFS configurations.</p></li>
<li><p>Provides data protection even in the case where only a single disk is available.</p></li>
<li><p>Allows you to select data protection policies on a per-file system basis, beyond the capabilities of the storage pool.</p></li></ul>
<p>Depending on the allocation of the ditto blocks in the storage pool, multiple
copies might be placed on a single disk. A subsequent full disk
failure might cause all ditto blocks to be unavailable.</p><p>You might consider using ditto blocks when you accidentally create a non-redundant pool
and when you need to set data retention policies.</p><p>For a detailed description of how setting copies on a system with
a single-disk pool or a multiple-disk pool might impact overall data protection, see this
blog:</p><p><a href="http://blogs.sun.com/relling/entry/zfs_copies_and_data_protection">http://blogs.sun.com/relling/entry/zfs_copies_and_data_protection</a></p><p>For more information about setting ZFS properties, see <a href="gayns.html#gazsp">Setting ZFS Properties</a>.</p>

<a name="gebws"></a><h4>Improved <tt>zpool status</tt> Output</h4>
<p><b>Solaris Express 1/07:</b> You can use the <tt>zpool status</tt> <tt>-v</tt> command to display a list of files
with persistent errors. Previously, you had to use the <tt>find</tt> <tt>-inum</tt> command to
identify the filenames from the list of displayed inodes.</p><p>For more information about displaying a list of files with persistent errors, see
<a href="gbbwl.html#gbctx">Repairing a Corrupted File or Directory</a>.</p>

<a name="gebwq"></a><h4>ZFS and Solaris iSCSI Improvements</h4>
<p><b>Solaris Express, Developer Edition 2/07:</b> In this Solaris release, you can create a ZFS volume as a
Solaris iSCSI target device by setting the <tt>shareiscsi</tt> property on the ZFS volume. This
method is a convenient way to quickly set up a Solaris iSCSI
target. For example:</p><pre># zfs create -V 2g tank/volumes/v2
# zfs set shareiscsi=on tank/volumes/v2
# iscsitadm list target
Target: tank/volumes/v2
    iSCSI Name: iqn.1986-03.com.sun:02:984fe301-c412-ccc1-cc80-cf9a72aa062a
    Connections: 0</pre><p>After the iSCSI target is created, set up the iSCSI initiator. For
information about setting up a Solaris iSCSI initiator, see <a href="http://docs.sun.com/doc/819-2723/fmvcd?a=view">Chapter 14, Configuring Solaris iSCSI Targets and Initiators (Tasks), in <i>System Administration Guide: Devices and File Systems</i></a>.</p><p>For more information about managing a ZFS volume as an iSCSI target, see
<a href="gaypf.html#gechv">Using a ZFS Volume as a Solaris iSCSI Target</a>.</p>

<a name="gebwp"></a><h4>Sharing ZFS File System Enhancements</h4>
<p><b>Solaris Express, Developer Edition 2/07:</b> In this Solaris release, the process of sharing file systems has been
improved. Although modifying system configuration files, such as <tt>/etc/dfs/dfstab</tt>, is unnecessary for sharing ZFS
file systems, you can use the <tt>sharemgr</tt> command to manage ZFS share properties.
The <tt>sharemgr</tt> command enables you to set and manage share properties on share
groups. ZFS shares are automatically designated in the <tt>zfs</tt> share group.</p><p>As in previous releases, you can set the ZFS <tt>sharenfs</tt> property on a
ZFS file system to share a ZFS file system. For example:</p><pre># zfs set sharenfs=on tank/home</pre><p>Or, you can use the new <tt>sharemgr</tt> <tt>add-share</tt> subcommand to share a ZFS
file system in the <tt>zfs</tt> share group. For example:</p><pre># sharemgr add-share -s tank/data zfs
# sharemgr show -vp zfs
zfs nfs=()
    zfs/tank/data
          /tank/data
          /tank/data/1
          /tank/data/2
          /tank/data/3</pre><p>Then, you can use the <tt>sharemgr</tt> command to manage ZFS shares. The following
example shows how to use <tt>sharemgr</tt> to set the <tt>nosuid</tt> property on the
shared ZFS file systems. You must preface ZFS share paths with <tt>/zfs</tt> designation.</p><pre># sharemgr set -P nfs -p nosuid=true zfs/tank/data
# sharemgr show -vp zfs
zfs nfs=()
    zfs/tank/data nfs=(nosuid="true")
          /tank/data
          /tank/data/1
          /tank/data/2
          /tank/data/3</pre><p>For more information, see <a href="http://docs.sun.com/doc/819-2240/sharemgr-1m?a=view"><tt>sharemgr</tt>(1M)</a>.</p>

<a name="gdswe"></a><h4>ZFS Command History (<tt>zpool history</tt>)</h4>
<p><b>Solaris Express 12/06:</b> In this Solaris release, ZFS automatically logs successful <tt>zfs</tt> and <tt>zpool</tt> commands
that modify pool state information. For example:<a name="indexterm-6"></a><a name="indexterm-7"></a><a name="indexterm-8"></a></p><pre># <tt><b>zpool history</b></tt>
History for 'newpool':
2007-04-25.11:37:31 zpool create newpool mirror c0t8d0 c0t10d0
2007-04-25.11:37:46 zpool replace newpool c0t10d0 c0t9d0
2007-04-25.11:38:04 zpool attach newpool c0t9d0 c0t11d0
2007-04-25.11:38:09 zfs create newpool/user1
2007-04-25.11:38:15 zfs destroy newpool/user1

History for 'tank':
2007-04-25.11:46:28 zpool create tank mirror c1t0d0 c2t0d0 mirror c3t0d0 c4t0d0</pre><p>This features enables you or Sun support personnel to identify the <b>exact</b>
set of ZFS commands that was executed to troubleshoot an error scenario.</p><p>You can identify a specific storage pool with the <tt>zpool history</tt> command. For example:</p><pre># <tt><b>zpool history newpool</b></tt>
History for 'newpool':
History for 'newpool':
2007-04-25.11:37:31 zpool create newpool mirror c0t8d0 c0t10d0
2007-04-25.11:37:46 zpool replace newpool c0t10d0 c0t9d0
2007-04-25.11:38:04 zpool attach newpool c0t9d0 c0t11d0
2007-04-25.11:38:09 zfs create newpool/user1
2007-04-25.11:38:15 zfs destroy newpool/user1</pre><p>The features of the history log are as follows:</p>
<ul><li><p>The log cannot be disabled.</p></li>
<li><p>The log is saved persistently on disk, which means the log is saved across system reboots.</p></li>
<li><p>The log is implemented as a ring buffer. The minimum size is 128 Kbytes. The maximum size is 32 Mbytes.</p></li>
<li><p>For smaller pools, the maximum size is capped at 1% of the pool size, where <i>size</i> is determined at pool creation time.</p></li>
<li><p>Requires no administration, which means tuning the size of the log or changing the location of the log is unnecessary.</p></li></ul>
<p>Currently, the <tt>zpool history</tt> command does not record  <i>user-ID</i>, <i>hostname</i>, or <i>zone-name</i>.</p><p>For more information about troubleshooting ZFS problems, see <a href="gbbuw.html">Identifying Problems in ZFS</a>.</p>

<a name="gdswf"></a><h4>ZFS Property Improvements</h4>


<a name="gebwr"></a><h5>ZFS <tt>xattr</tt> Property</h5>
<p><b>Solaris Express 1/07:</b> You can use the <tt>xattr</tt> property to disable or enable extended attributes for
a specific ZFS file system. The default value is on. For a description
of ZFS properties, see <a href="gazss.html">Introducing ZFS Properties</a>.</p>

<a name="gdswd"></a><h5>ZFS <tt>canmount</tt> Property</h5>
<p><b>Solaris Express 10/06:</b> The new <tt>canmount</tt> property allows you to specify whether a dataset can be
mounted by using the <tt>zfs mount</tt> command. For more information, see <a href="gazss.html#gdrcf">The <tt>canmount</tt> Property</a>.</p>

<a name="gdsus"></a><h5>ZFS User Properties</h5>
<p><b>Solaris Express 10/06:</b> In addition to the standard native properties that can either export internal statistics
or control ZFS file system behavior, ZFS supports user properties. User properties have
no effect on ZFS behavior, but you can use them to annotate datasets
with information that is meaningful in your environment.</p><p>For more information, see <a href="gazss.html#gdrcw">ZFS User Properties</a>.</p>

<a name="gdsvx"></a><h5>Setting Properties When Creating ZFS File Systems</h5>
<p><b>Solaris Express 10/06:</b> In this Solaris release, you can set properties when you create a
file system, in addition to setting properties after the file system is created.</p><p>The following examples illustrate equivalent syntax:</p><pre># <tt><b>zfs create tank/home</b></tt>
# <tt><b>zfs set mountpoint=/export/zfs tank/home</b></tt>
# <tt><b>zfs set sharenfs=on tank/home</b></tt>
# <tt><b>zfs set compression=on tank/home</b></tt></pre><pre># <tt><b>zfs create -o mountpoint=/export/zfs -o sharenfs=on -o compression=on tank/home</b></tt></pre>

<a name="gdsvh"></a><h4>Displaying All ZFS File System Information</h4>
<p><b>Solaris Express 10/06:</b> In this Solaris release, you can use various forms of the <tt>zfs get</tt>
command to display information about all datasets if you do not specify a
dataset. In previous releases, all dataset information was not retreivable with the <tt>zfs get</tt>
command.</p><p>For example:</p><pre># <tt><b>zfs get -s local all</b></tt>
tank/home               atime          off                    local
tank/home/bonwick       atime          off                    local
tank/home/marks         quota          50G                    local</pre>

<a name="gdsup"></a><h4>New <tt>zfs receive</tt> <tt>-F</tt> Option</h4>
<p><b>Solaris Express 10/06:</b> In this Solaris release, you can use the new <tt>-F</tt> option to
the <tt>zfs receive</tt> command to force a rollback of the file system to
the most recent snapshot before doing the receive. Using this option might be necessary
when the file system is modified between the time a rollback occurs and
the receive is initiated.</p><p>For more information, see <a href="gbchx.html#gbimy">Restoring a ZFS Snapshot</a>.</p>

<a name="gdfdt"></a><h4>Recursive ZFS Snapshots</h4>
<p><b>Solaris Express 8/06:</b> When you use the <tt>zfs snapshot</tt> command to create a file system snapshot, you
can use the <tt>-r</tt> option to recursively create snapshots for all descendent file
systems. In addition, using the <tt>-r</tt> option recursively destroys all descendent snapshots when
a snapshot is destroyed.</p><p>Recursive ZFS snapshots are created quickly as one atomic operation. The snapshots are
created together (all at once) or not created at all. The benefit of
atomic snapshots operations is that the snapshot data is always taken at one
consistent time, even across descendent file systems.</p><p>For more information, see <a href="gbciq.html#gbcya">Creating and Destroying ZFS Snapshots</a>.</p>

<a name="gcviu"></a><h4>Double Parity RAID-Z (<tt>raidz2</tt>)</h4>
<p><b>Solaris Express 7/06:</b> A redundant RAID-Z configuration can now have either single- or double-parity, which means
that one or two device failures can be sustained respectively, without any data
loss. You can specify the <tt>raidz2</tt> keyword for a double-parity RAID-Z configuration. Or,
 you can specify the <tt>raidz</tt> or <tt>raidz1</tt> keyword for a single-parity
RAID-Z configuration.</p><p>For more information, see <a href="gaypw.html#gcvjg">Creating RAID-Z Storage Pools</a> or <a href="http://docs.sun.com/doc/819-2240/zpool-1m?a=view"><tt>zpool</tt>(1M)</a>.</p>

<a name="gcvdm"></a><h4>Hot Spares for ZFS Storage Pool Devices</h4>
<p><b>Solaris Express 7/06:</b> The ZFS hot spares feature enables you to identify disks that could
be used to replace a failed or faulted device in one or more
storage pools. Designating a device as a <b>hot spare</b> means that if an active
device in the pool fails, the hot spare automatically replaces the failed device. Or,
you can manually replace a device in a storage pool with a
hot spare.</p><p>For more information, see <a href="gayrd.html#gcvcw">Designating Hot Spares in Your Storage Pool</a> and <a href="http://docs.sun.com/doc/819-2240/zpool-1m?a=view"><tt>zpool</tt>(1M)</a>.</p>

<a name="gcvgc"></a><h4>Replacing a ZFS File System With a ZFS Clone (<tt>zfs promote</tt>)</h4>
<p><b>Solaris Express 7/06:</b> The <tt>zfs promote</tt> command enables you to replace an existing ZFS file system
with a clone of that file system. This feature is helpful when
you want to run tests on an alternative version of a file system
and then, make that alternative version of the file system the active file
system.</p><p>For more information, see <a href="gbcxz.html#gcvfl">Replacing a ZFS File System With a ZFS Clone</a> and <a href="http://docs.sun.com/doc/819-2240/zfs-1m?a=view"><tt>zfs</tt>(1M)</a>.</p>

<a name="gcvit"></a><h4>Upgrading ZFS Storage Pools (<tt>zpool upgrade</tt>)</h4>
<p><b>Solaris Express 6/06:</b> You can upgrade your storage pools to a newer version to take
advantage of the latest features by using the <tt>zpool upgrade</tt> command. In addition, the <tt>zpool status</tt>
command has been modified to notify you when your pools are running older
versions.</p><p>For more information, see <a href="gbchy.html#gcikw">Upgrading ZFS Storage Pools</a> and <a href="http://docs.sun.com/doc/819-2240/zpool-1m?a=view"><tt>zpool</tt>(1M)</a>.</p><p>If you want to use the ZFS Administration console on a system
with a pool from a previous Solaris release, make sure you upgrade your
pools before using the ZFS Administration console. To see if your pools need
to be upgraded, use the <tt>zpool status</tt> command. For information about the ZFS Administration console,
see <a href="#gbsbp">ZFS Web-Based Management</a>.</p>

<a name="gcsxk"></a><h4>Using ZFS to Clone Non-Global Zones and Other Enhancements</h4>
<p><b>Solaris Express 6/06:</b> When the source <tt>zonepath</tt> and the target <tt>zonepath</tt> both reside on ZFS and
are in the same pool, <tt>zoneadm clone</tt> now automatically uses the ZFS  clone
feature to clone a zone. This enhancement means that <tt>zoneadm clone</tt> will take a
ZFS snapshot of the source <tt>zonepath</tt> and set up the target <tt>zonepath</tt>. The snapshot
is named <tt>SUNWzoneX</tt>, where <tt>X</tt> is a unique ID used to distinguish between
multiple snapshots. The destination zone's <tt>zonepath</tt> is used to name the ZFS clone.
A software inventory is performed so that a snapshot used at a future
time can be validated by the system. Note that you can still specify
that the ZFS <tt>zonepath</tt> be copied instead of the ZFS clone, if desired.</p><p>To clone a source zone multiple times, a new parameter added to
<tt>zoneadm</tt> allows you to specify that an existing snapshot should be used. The
system validates that the existing snapshot is usable on the target. Additionally, the
zone install process now has the capability to detect when a ZFS
file system can be created for a zone, and the uninstall process can
detect when a ZFS file system in a zone can be destroyed. These
steps are then performed automatically by the <tt>zoneadm</tt> command.</p><p>Keep the following points in mind when using ZFS on a system
with Solaris containers installed:</p>
<ul><li><p>Do not use the ZFS snapshot features to clone a zone</p></li>
<li><p>You can delegate or a add a ZFS file system to a non-global zone. For more information, see <a href="gayov.html#gbbrq">Adding ZFS File Systems to a Non-Global Zone</a> or <a href="gayov.html#gbbst">Delegating Datasets to a Non-Global Zone</a>.</p></li>
<li><p>Do not use a ZFS file system for a global zone root path or a non-global zone root path in the Solaris 10 releases. You can use ZFS as a zone root path in the Solaris Express releases, but keep in mind that patching or upgrading these zones is not supported.</p></li></ul>
<p>For more information, see <a href="http://docs.sun.com/doc/819-2450"><i>System Administration Guide: Virtualization Using the Solaris Operating System</i></a>.</p>

<a name="gciui"></a><h4>ZFS Backup and Restore Commands are Renamed</h4>
<p><b>Solaris Express 5/06:</b> In this Solaris release, the <tt>zfs backup</tt> and <tt>zfs restore</tt> commands are renamed to <tt>zfs send</tt>
and <tt>zfs receive</tt> to more accurately describe their function. The function of these commands
is to save and restore ZFS data stream representations.</p><p>For more information about these commands, see <a href="gbchx.html">Saving and Restoring ZFS Data</a>.</p>

<a name="gcitn"></a><h4>Recovering Destroyed Storage Pools</h4>
<p><b>Solaris Express 5/06:</b> This release includes the <tt>zpool import</tt> <tt>-D</tt> command, which enables you to recover pools
that were previously destroyed with the <tt>zpool destroy</tt> command.</p><p>For more information, see <a href="gbchy.html#gcfhw">Recovering Destroyed ZFS Storage Pools</a>.</p>

<a name="gcfhy"></a><h4>ZFS is Integrated With Fault Manager</h4>
<p><b>Solaris Express 4/06:</b> This release includes the integration of a ZFS diagnostic engine that is
capable of diagnosing and reporting pool failures and device failures. Checksum, I/O, device, and
pool errors associated with pool or device failures are also reported.</p><p>The diagnostic engine does not include predictive analysis of  checksum and I/O
errors, nor does it include proactive actions based on fault analysis.</p><p>In the event of the ZFS failure, you might see a message
similar to the following from <tt>fmd</tt>:</p><pre>SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major
EVENT-TIME: Fri Mar 10 11:09:06 MST 2006
PLATFORM: SUNW,Ultra-60, CSN: -, HOSTNAME: neo
SOURCE: zfs-diagnosis, REV: 1.0
EVENT-ID: b55ee13b-cd74-4dff-8aff-ad575c372ef8
DESC: A ZFS device failed.  Refer to http://sun.com/msg/ZFS-8000-D3 for more information.
AUTO-RESPONSE: No automated response will occur.
IMPACT: Fault tolerance of the pool may be compromised.
REC-ACTION: Run 'zpool status -x' and replace the bad device.</pre><p>By reviewing the recommended action, which will be to follow the more specific
directions in the <tt>zpool status</tt> command, you will be able to quickly identify and
resolve the failure. </p><p>For an example of recovering from a reported ZFS problem, see <a href="gbbvb.html">Repairing a Missing Device</a>.</p>

<a name="gcfiw"></a><h4>New <tt>zpool clear</tt> Command</h4>
<p><b>Solaris Express 4/06:</b> This release includes the <tt>zpool clear</tt> command for clearing error counts associated with a
device or the pool. Previously, error counts were cleared when a device in
a pool was brought online with the <tt>zpool online</tt> command. For more information,
see <a href="http://docs.sun.com/doc/819-2240/zpool-1m?a=view"><tt>zpool</tt>(1M)</a> and <a href="gayrd.html#gazge">Clearing Storage Pool Devices</a>.</p>

<a name="gcajn"></a><h4>Compact NFSv4 ACL Format</h4>
<p><b>Solaris Express 4/06:</b> In this release, three NFSv4 ACL formats are available: verbose, positional, and compact.
The new compact and positional ACL formats are available to set and display
ACLs. You can use the <tt>chmod</tt> command to set all 3 ACL formats.
You can use the <tt>ls</tt> <tt>-V</tt> command to display compact and positional ACL
formats and the <tt>ls</tt> <tt>-v</tt> command to display verbose ACL formats.</p><p>For more information, see <a href="gbchf.html">Setting and Displaying ACLs on ZFS Files in Compact Format</a>, <a href="http://docs.sun.com/doc/819-2239/chmod-1?a=view"><tt>chmod</tt>(1)</a>, and <a href="http://docs.sun.com/doc/819-2239/ls-1?a=view"><tt>ls</tt>(1)</a>.</p>

<a name="gcakl"></a><h4>File System Monitoring Tool (<tt>fsstat</tt>)</h4>
<p><b>Solaris Express 4/06:</b> A new file system monitoring tool, <tt>fsstat</tt>, is available to report file system
operations. Activity can be reported by mount point or by file system type.
 The following example shows general ZFS file system activity.</p><pre>$ <tt><b>fsstat zfs</b></tt>
 new  name   name  attr  attr lookup rddir  read read  write write
 file remov  chng   get   set    ops   ops   ops bytes   ops bytes
7.82M 5.92M 2.76M 1.02G 3.32M  5.60G 87.0M  363M 1.86T 20.9M  251G zfs</pre><p>For more information, see <a href="http://docs.sun.com/doc/819-2240/fsstat-1m?a=view"><tt>fsstat</tt>(1M)</a>.</p>

<a name="gbsbp"></a><h4>ZFS Web-Based Management</h4>
<p><b>Solaris Express 1/06:</b> A web-based ZFS management tool is available to perform many administrative actions. With
this tool, you can perform the following tasks:</p>
<ul><li><p>Create a new storage pool.</p></li>
<li><p>Add capacity to an existing pool.</p></li>
<li><p>Move (export) a storage pool to another system.</p></li>
<li><p>Import a previously exported storage pool to make it available on another system.</p></li>
<li><p>View information about storage pools.</p></li>
<li><p>Create a file system.</p></li>
<li><p>Create a volume.</p></li>
<li><p>Take a snapshot of a file system or a volume.</p></li>
<li><p>Roll back a file system to a previous snapshot.</p></li></ul>
<p>You can access the ZFS Administration console through a secure web browser at
the following URL:</p><pre>https://<i>system-name</i>:6789/zfs</pre><p>If you type the appropriate URL and are unable to reach the
ZFS Administration console, the server might not be started. To start the server,
run the following command:</p><pre># /usr/sbin/smcwebserver start</pre><p>If you want the server to run automatically when the system boots,
run the following command:</p><pre># /usr/sbin/smcwebserver enable</pre>
<hr><p><b>Note - </b>You cannot use the Solaris Management Console (<tt>smc</tt>) to manage ZFS storage pools
or file systems.</p>
<hr>
<p>You will not be able to manage ZFS file systems remotely with the
ZFS Administration console because of a change in a recent Solaris release, which
shutdown some network services automatically. Use the following command to enable these services:</p><pre># <tt><b>netservices open</b></tt></pre>
         </div>
      </td>
   </tr>

   <tr class="PageControls" valign="top">
      <td></td>
      <td>
         <table width="100%">
      	   <tr>
      	     <td>
                 <a href="zfsover-1.html">Previous</a>
             </td>
             <td align="right">
                 <a href="zfsover-2.html">Next</a>
             </td>
           </tr>
         </table>
      </td>
   </tr>
</tbody>
</table>


</body>
</html>

